# Plan d'Impl√©mentation RAG - Version R√©vis√©e (Post-Audit)

**Version:** 4.0 (Production-Ready)
**Date:** 14 novembre 2025
**Statut:** ‚úÖ **Valid√© par Audits Architecture + UX**
**Timeline:** 24 jours (r√©aliste)
**Budget:** ‚Ç¨10,560 + $78 API/100 docs

---

## üéØ R√©sum√© Ex√©cutif

### Changements Critiques vs Plan Original

| Aspect | Plan v1.0 | Plan v4.0 (R√©vis√©) | Impact |
|--------|-----------|-------------------|--------|
| **Timeline** | 15 jours | **24 jours** | +60% (r√©aliste) |
| **Phase 0.5** | Aucune | **3 jours** (corrections critiques) | BLOQUANT |
| **Strat√©gie Pinecone** | $or/$contains | **Dual queries** | Architecture compl√®te r√©√©crite |
| **Co√ªts API** | $7-15 | **$47-78** | 5-11√ó plus √©lev√© (r√©el) |
| **Metadata Schema** | Incomplet | **Migration compl√®te** | Nouveau champs requis |
| **Faisabilit√©** | 3/10 üî¥ | **8/10 ‚úÖ** | Impl√©mentable |

### Corrections Critiques Appliqu√©es

1. ‚úÖ **Pinecone Filtering** - Dual queries au lieu de $or/$contains (impossible)
2. ‚úÖ **Schema Migration** - Ajout de documentPurpose, contentTypeTags, isHistoricalRfp
3. ‚úÖ **Multi-tenant** - Uniformisation sur tenant_id (s√©curit√©)
4. ‚úÖ **Embedding Model** - Uniformisation sur text-embedding-3-small
5. ‚úÖ **Cost Estimation** - Budget r√©aliste avec tous les co√ªts
6. ‚úÖ **UX/Accessibility** - WCAG 2.1 AA compliance
7. ‚úÖ **Timeline** - Ajout buffer pour risques

---

## üö® Phase 0.5: Corrections Critiques (3 jours)

> **OBLIGATOIRE avant toute impl√©mentation - Risque d'√©chec 90% sans cette phase**

### Jour 1: Architecture Pinecone

**Objectif:** Valider que la strat√©gie dual queries fonctionne

```typescript
// POC: Dual Queries Strategy
async function testDualQueriesPerformance() {
  const namespace = pineconeIndex.namespace('rfp-library-production');

  // Query 1: Support docs
  const start1 = Date.now();
  const supportResults = await namespace.query({
    vector: testEmbedding,
    topK: 5,
    filter: {
      tenant_id: { $eq: 'test-company' },
      documentPurpose: { $eq: 'rfp_support' },
      contentTypeTags: { $in: ['methodology'] }
    }
  });
  const latency1 = Date.now() - start1;

  // Query 2: Historical RFPs
  const start2 = Date.now();
  const historicalResults = await namespace.query({
    vector: testEmbedding,
    topK: 5,
    filter: {
      tenant_id: { $eq: 'test-company' },
      isHistoricalRfp: { $eq: true }
    }
  });
  const latency2 = Date.now() - start2;

  // Benchmark
  console.log(`Support query: ${latency1}ms`);
  console.log(`Historical query: ${latency2}ms`);
  console.log(`Total: ${latency1 + latency2}ms`);

  // Validate < 300ms total
  assert(latency1 + latency2 < 300, 'Latency too high');

  // Test merge + dedup
  const merged = mergeAndRankResults(supportResults, historicalResults);
  console.log(`Merged results: ${merged.length}`);
}
```

**T√¢ches:**
- [ ] Cr√©er POC dual queries
- [ ] Benchmarker avec 1K, 10K, 50K vectors
- [ ] Valider performance <300ms (P95)
- [ ] Documenter limites scaling

**D√©cision √† prendre:** Namespace unique vs s√©par√©?
- **Option A:** Namespace unique `rfp-library` (recommand√© - simplicit√©)
- **Option B:** Namespaces s√©par√©s `rfp-support` + `rfp-responses` (scaling >100 companies)

**Livrable:** Document de validation technique + d√©cision architecture

---

### Jour 2: Migration Base de Donn√©es

**Objectif:** Ajouter champs manquants sans casser l'existant

#### 2.1 Migration Drizzle (2h)

```typescript
// migrations/0008_add_support_docs_fields.ts
import { sql } from 'drizzle-orm';
import { pgTable, varchar, boolean, text } from 'drizzle-orm/pg-core';

export async function up(db) {
  // Ajouter nouvelles colonnes
  await db.execute(sql`
    ALTER TABLE documents
    ADD COLUMN IF NOT EXISTS document_purpose VARCHAR(50),
    ADD COLUMN IF NOT EXISTS content_type VARCHAR(100),
    ADD COLUMN IF NOT EXISTS content_type_tags TEXT[],
    ADD COLUMN IF NOT EXISTS is_historical_rfp BOOLEAN DEFAULT FALSE;
  `);

  // Backfill pour documents existants
  await db.execute(sql`
    UPDATE documents
    SET
      document_purpose = 'rfp_response',
      is_historical_rfp = (rfp_id IS NOT NULL),
      content_type_tags = ARRAY['legacy']::TEXT[]
    WHERE document_purpose IS NULL;
  `);

  // Indexes pour performance
  await db.execute(sql`
    CREATE INDEX IF NOT EXISTS idx_documents_purpose
    ON documents(document_purpose);

    CREATE INDEX IF NOT EXISTS idx_documents_content_tags
    ON documents USING GIN(content_type_tags);

    CREATE INDEX IF NOT EXISTS idx_documents_historical
    ON documents(is_historical_rfp)
    WHERE is_historical_rfp = TRUE;
  `);

  // Contraintes
  await db.execute(sql`
    ALTER TABLE documents
    ADD CONSTRAINT check_document_purpose
    CHECK (document_purpose IN ('rfp_response', 'rfp_support', 'company_info'));
  `);
}

export async function down(db) {
  await db.execute(sql`
    DROP INDEX IF EXISTS idx_documents_purpose;
    DROP INDEX IF EXISTS idx_documents_content_tags;
    DROP INDEX IF EXISTS idx_documents_historical;

    ALTER TABLE documents
    DROP CONSTRAINT IF EXISTS check_document_purpose,
    DROP COLUMN IF EXISTS document_purpose,
    DROP COLUMN IF EXISTS content_type,
    DROP COLUMN IF EXISTS content_type_tags,
    DROP COLUMN IF EXISTS is_historical_rfp;
  `);
}
```

**T√¢ches:**
- [ ] Cr√©er migration Drizzle
- [ ] Tester sur base de dev (backup first!)
- [ ] Valider backward compatibility
- [ ] Tester rollback (down migration)
- [ ] Appliquer sur staging

#### 2.2 Update TypeScript Interfaces (2h)

```typescript
// src/db/schema.ts
export const documents = pgTable('documents', {
  id: uuid('id').primaryKey().defaultRandom(),
  companyId: uuid('company_id').notNull().references(() => companies.id),
  name: varchar('name', { length: 255 }).notNull(),

  // NOUVEAUX CHAMPS
  documentPurpose: varchar('document_purpose', { length: 50 })
    .$type<'rfp_response' | 'rfp_support' | 'company_info'>()
    .notNull(),
  contentType: varchar('content_type', { length: 100 }),
  contentTypeTags: text('content_type_tags').array(),
  isHistoricalRfp: boolean('is_historical_rfp').default(false),

  // Existants...
  s3Key: varchar('s3_key', { length: 500 }).notNull(),
  metadata: jsonb('metadata').$type<DocumentMetadata>(),
  createdAt: timestamp('created_at').defaultNow(),
});

// src/lib/rfp/pinecone.ts
export interface RFPVectorMetadata {
  // Core
  documentId: string;
  tenant_id: string;  // ‚úÖ RENOMM√â de companyId
  documentType: string;

  // NOUVEAUX
  documentPurpose?: 'rfp_support' | 'rfp_response' | 'company_info';
  contentType?: string;
  contentTypeTags?: string[];
  isHistoricalRfp?: boolean;
  category?: string;

  // Scoring
  qualityScore?: number;
  timesUsed?: number;
  lastUsedAt?: string;

  // Historique
  sourceRfpId?: string;
  outcomeScore?: number;
  createdAt: number;
}
```

**T√¢ches:**
- [ ] Update schema.ts
- [ ] Update pinecone.ts interfaces
- [ ] Find & replace `companyId` ‚Üí `tenant_id` dans filters
- [ ] Update tests
- [ ] Valider TypeScript errors = 0

#### 2.3 Tests Backward Compatibility (2h)

```typescript
// tests/migrations/backward-compatibility.test.ts
describe('Schema Migration Backward Compatibility', () => {
  it('should handle legacy documents without new fields', async () => {
    const legacyDoc = await db.query.documents.findFirst({
      where: eq(documents.document_purpose, null)
    });

    // Should not crash
    expect(legacyDoc).toBeDefined();
  });

  it('should backfill historical RFPs correctly', async () => {
    const historicalDocs = await db.query.documents.findMany({
      where: and(
        isNotNull(documents.rfpId),
        eq(documents.is_historical_rfp, true)
      )
    });

    expect(historicalDocs.length).toBeGreaterThan(0);
  });

  it('should preserve existing Pinecone vectors', async () => {
    const namespace = pineconeIndex.namespace('rfp-library-production');
    const stats = await namespace.describeIndexStats();

    // Aucune perte de vecteurs
    expect(stats.totalVectorCount).toBeGreaterThan(1000);
  });
});
```

**Livrable:** Migration valid√©e en staging + tests passent

---

### Jour 3: Uniformisation & Validation

**Objectif:** Corriger inconsistances critiques

#### 3.1 Uniformisation tenant_id (3h)

**Find & Replace dans tous les fichiers:**

```bash
# Rechercher tous les usages
rg "companyId.*filter" --type ts

# Remplacer
sed -i '' 's/companyId: { \$eq:/tenant_id: { $eq:/g' src/**/*.ts
```

**Fichiers critiques:**
- `src/lib/rag/engine.ts` (ligne 196, 211)
- `src/app/api/rfps/[id]/questions/[questionId]/generate-response/route.ts`
- Tous les tests avec Pinecone queries

**Validation:**
```typescript
// tests/security/multi-tenant-isolation.test.ts
describe('Multi-tenant isolation', () => {
  it('should never leak data across companies', async () => {
    const companyA = 'company-a';
    const companyB = 'company-b';

    // Query company A
    const resultsA = await namespace.query({
      vector: testEmbedding,
      topK: 100,
      filter: { tenant_id: { $eq: companyA } }
    });

    // Validate NO results from company B
    resultsA.matches.forEach(match => {
      expect(match.metadata.tenant_id).toBe(companyA);
      expect(match.metadata.tenant_id).not.toBe(companyB);
    });
  });
});
```

**T√¢ches:**
- [ ] Find & replace companyId ‚Üí tenant_id
- [ ] Update tous les tests
- [ ] Run security tests
- [ ] Validation manuelle (code review)

#### 3.2 Correction Mod√®les Claude (30min)

```typescript
// src/lib/constants/ai-models.ts
export const CLAUDE_MODELS = {
  sonnet: 'claude-sonnet-4-5-20250929',
  haiku: 'claude-4-5-haiku-20250514', // ‚úÖ CORRIG√â (√©tait claude-haiku-4-20250514)
} as const;

// Validation
import Anthropic from '@anthropic-ai/sdk';
const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY });

// Test que les mod√®les existent
const response = await anthropic.messages.create({
  model: CLAUDE_MODELS.haiku,
  max_tokens: 10,
  messages: [{ role: 'user', content: 'test' }]
});

console.log('‚úÖ Claude Haiku model validated');
```

#### 3.3 Uniformisation Embeddings (2h)

**D√©cision:** Uniformiser sur `text-embedding-3-small` (recommand√© - co√ªts 70% moins cher)

```typescript
// src/lib/constants/ai-models.ts
export const EMBEDDING_CONFIG = {
  model: 'text-embedding-3-small',
  dimensions: 1536, // Match Pinecone index
  batchSize: 100,
} as const;

// Update engine.ts
const embeddingResponse = await openai.embeddings.create({
  model: EMBEDDING_CONFIG.model,
  input: batchChunks.map(c => c.content),
});

// Update generate-response/route.ts
const queryEmbedding = await openai.embeddings.create({
  model: EMBEDDING_CONFIG.model, // ‚úÖ Uniformis√© (√©tait text-embedding-3-large)
  input: sanitizedQuestion,
});
```

**Migration Pinecone si n√©cessaire:**
- Si index actuel = 3072 dims (large) ‚Üí Cr√©er nouveau index 1536 dims
- Re-embed tous les documents (co√ªt: ~$50 pour 10K chunks)
- Swap namespace

**T√¢ches:**
- [ ] V√©rifier dimension actuelle Pinecone
- [ ] Si 3072 dims ‚Üí Planifier migration
- [ ] Si 1536 dims ‚Üí Juste uniformiser le code
- [ ] Update all embedding calls

**Livrable:** Codebase unifi√©, pr√™t pour Phase 1

---

## üìã Phase 1: Backend Core (4 jours)

### Jour 4: Document Analysis Service

**Objectif:** Service d'auto-cat√©gorisation avec Claude

```typescript
// src/lib/rfp/services/document-analysis.service.ts
import { CLAUDE_MODELS } from '@/lib/constants/ai-models';

export interface DocumentAnalysis {
  documentType: string;
  confidence: number;
  suggestedCategories: Array<{
    category: string;
    confidence: number;
  }>;
  contentTypeTags: string[];
  executiveSummary: string;
}

export async function analyzeDocument(
  extractedText: string,
  filename: string
): Promise<DocumentAnalysis> {
  const prompt = `Tu es un expert en analyse de documents d'entreprise.

Analyse ce document et fournis en JSON:

{
  "documentType": "methodology_guide" | "case_study" | "technical_spec" | "certification" | "financial_info" | "company_overview" | "other",
  "confidence": 0.95,
  "suggestedCategories": [
    { "category": "project-methodology", "confidence": 0.92 },
    { "category": "team-structure", "confidence": 0.78 }
  ],
  "contentTypeTags": ["agile", "scrum", "sprint-planning"],
  "executiveSummary": "Ce document d√©crit..."
}

Cat√©gories RFP disponibles:
- project-methodology
- technical-solution
- team-structure
- case-study
- certifications
- financial-info
- legal-compliance
- product-catalog
- company-overview

Filename: ${filename}
Content (first 100K chars):
${extractedText.slice(0, 100000)}`;

  const response = await anthropic.messages.create({
    model: CLAUDE_MODELS.haiku, // Haiku = fast + cheap
    max_tokens: 4096,
    temperature: 0.3,
    messages: [{ role: 'user', content: prompt }]
  });

  const content = response.content[0];
  if (content.type !== 'text') throw new Error('Unexpected response type');

  const jsonMatch = content.text.match(/\{[\s\S]*\}/);
  if (!jsonMatch) throw new Error('Failed to parse JSON response');

  const result: DocumentAnalysis = JSON.parse(jsonMatch[0]);

  // Validation
  if (!result.documentType || !result.suggestedCategories) {
    throw new Error('Invalid analysis result');
  }

  return result;
}
```

**T√¢ches:**
- [ ] Impl√©menter service d'analyse
- [ ] Tests avec 10 types de documents diff√©rents
- [ ] Fallback si Claude timeout (>30s)
- [ ] Retry avec Sonnet si Haiku confidence <0.7
- [ ] Caching des analyses (√©viter duplicatas)

**Co√ªts estim√©s:**
- Haiku: 100 docs √ó 20K tokens input √ó $0.00025/1K = **$0.50**
- Retry Sonnet (30%): 30 docs √ó 20K √ó $0.003/1K = **$1.80**
- **Total: $2.30 / 100 docs**

---

### Jour 5: Upload API + S3 Integration

**Objectif:** Endpoint d'upload avec validation

```typescript
// src/app/api/knowledge-base/upload/route.ts
import { z } from 'zod';
import { S3Client, PutObjectCommand } from '@aws-sdk/client-s3';

const uploadSchema = z.object({
  file: z.instanceof(File),
  documentPurpose: z.enum(['rfp_support', 'rfp_response']).default('rfp_support'),
  contentType: z.string().optional(),
  tags: z.array(z.string()).optional(),
});

export async function POST(request: Request) {
  const session = await getServerSession();
  if (!session?.user?.companyId) {
    return new Response('Unauthorized', { status: 401 });
  }

  const formData = await request.formData();
  const file = formData.get('file') as File;

  // Validation
  const parsed = uploadSchema.safeParse({
    file,
    documentPurpose: formData.get('documentPurpose'),
    contentType: formData.get('contentType'),
    tags: formData.getAll('tags'),
  });

  if (!parsed.success) {
    return Response.json({ error: parsed.error }, { status: 400 });
  }

  // Upload to S3
  const s3Key = `companies/${session.user.companyId}/support-docs/${crypto.randomUUID()}-${file.name}`;
  const s3Client = new S3Client({ region: process.env.AWS_REGION });

  await s3Client.send(new PutObjectCommand({
    Bucket: process.env.S3_BUCKET_NAME,
    Key: s3Key,
    Body: Buffer.from(await file.arrayBuffer()),
    ContentType: file.type,
  }));

  // Create document record
  const document = await db.insert(documents).values({
    companyId: session.user.companyId,
    name: file.name,
    type: file.type,
    s3Key,
    s3Bucket: process.env.S3_BUCKET_NAME,
    documentPurpose: parsed.data.documentPurpose,
    contentType: parsed.data.contentType,
    contentTypeTags: parsed.data.tags,
    isHistoricalRfp: false,
  }).returning();

  // Trigger async analysis
  await analyzeDocumentAsync(document[0].id);

  return Response.json({ documentId: document[0].id });
}
```

**T√¢ches:**
- [ ] Impl√©menter upload endpoint
- [ ] Validation fichiers (PDF, DOCX max 50MB)
- [ ] Rate limiting (20 uploads/heure)
- [ ] Tests avec diff√©rents formats
- [ ] Error handling (S3 down, disk full)

---

### Jour 6-7: Dual Retrieval Engine

**Objectif:** Impl√©mentation compl√®te de la strat√©gie dual queries

```typescript
// src/lib/rag/retrieval/dual-query-engine.ts
export class DualQueryRetrievalEngine {
  constructor(
    private pineconeIndex: PineconeIndex,
    private namespace: string
  ) {}

  async retrieve(
    queryEmbedding: number[],
    category: string,
    companyId: string,
    options: {
      pinnedSourceRfpId?: string;
      depth?: 'basic' | 'detailed' | 'comprehensive';
    }
  ): Promise<RetrievalResult> {
    const { pinnedSourceRfpId, depth = 'detailed' } = options;
    const topK = depth === 'basic' ? 5 : depth === 'detailed' ? 10 : 20;
    const ns = this.pineconeIndex.namespace(this.namespace);

    // PHASE 1: Pinned Source (si sp√©cifi√©)
    let pinnedResults: ScoredVector[] = [];
    if (pinnedSourceRfpId) {
      pinnedResults = await ns.query({
        vector: queryEmbedding,
        topK: Math.ceil(topK * 0.4), // 40% du budget
        filter: {
          tenant_id: { $eq: companyId },
          documentPurpose: { $eq: 'rfp_response' },
          sourceRfpId: { $eq: pinnedSourceRfpId }
        },
        includeMetadata: true
      });
    }

    // PHASE 2A: Support Documents
    const supportResults = await ns.query({
      vector: queryEmbedding,
      topK: Math.ceil(topK * 0.3),
      filter: {
        tenant_id: { $eq: companyId },
        documentPurpose: { $eq: 'rfp_support' },
        contentTypeTags: { $in: [category, 'general'] }
      },
      includeMetadata: true
    });

    // PHASE 2B: Historical RFPs
    const historicalResults = await ns.query({
      vector: queryEmbedding,
      topK: Math.ceil(topK * 0.3),
      filter: {
        tenant_id: { $eq: companyId },
        documentPurpose: { $eq: 'rfp_response' },
        isHistoricalRfp: { $eq: true }
      },
      includeMetadata: true
    });

    // Merge + Deduplicate + Score
    const allResults = [
      ...pinnedResults.map(r => ({ ...r, source: 'pinned' as const, boost: 1.5 })),
      ...supportResults.map(r => ({ ...r, source: 'support' as const, boost: 1.2 })),
      ...historicalResults.map(r => ({ ...r, source: 'historical' as const, boost: 1.0 }))
    ];

    const deduped = this.deduplicateByChunk(allResults);
    const scored = this.calculateCompositeScores(deduped);

    scored.sort((a, b) => b.compositeScore - a.compositeScore);

    return {
      chunks: scored.slice(0, topK),
      sources: this.groupByDocument(scored),
      metadata: {
        totalResults: scored.length,
        pinnedCount: pinnedResults.length,
        supportCount: supportResults.length,
        historicalCount: historicalResults.length
      }
    };
  }

  private deduplicateByChunk(results: any[]): any[] {
    const seen = new Set<string>();
    return results.filter(r => {
      const key = `${r.metadata.documentId}:${r.metadata.chunkIndex}`;
      if (seen.has(key)) return false;
      seen.add(key);
      return true;
    });
  }

  private calculateCompositeScores(results: any[]): any[] {
    return results.map(result => {
      const semanticScore = result.score; // 0-1
      const outcomeScore = result.metadata.outcomeScore || 0.5;
      const recencyScore = this.calculateRecencyScore(result.metadata.createdAt);
      const qualityScore = result.metadata.qualityScore || 0.7;
      const sourceBoost = result.boost;

      const compositeScore =
        (semanticScore * 0.40 +
        outcomeScore * 0.25 +
        recencyScore * 0.15 +
        qualityScore * 0.20) * sourceBoost;

      return {
        ...result,
        compositeScore,
        breakdown: { semanticScore, outcomeScore, recencyScore, qualityScore, sourceBoost }
      };
    });
  }

  private calculateRecencyScore(createdAt: number): number {
    const ageInDays = (Date.now() - createdAt) / (1000 * 60 * 60 * 24);
    const halfLife = 180; // 6 mois
    return Math.exp(-Math.log(2) * ageInDays / halfLife);
  }

  private groupByDocument(results: any[]): Map<string, any[]> {
    const grouped = new Map<string, any[]>();
    for (const result of results) {
      const docId = result.metadata.documentId;
      if (!grouped.has(docId)) {
        grouped.set(docId, []);
      }
      grouped.get(docId)!.push(result);
    }
    return grouped;
  }
}
```

**T√¢ches:**
- [ ] Impl√©menter DualQueryRetrievalEngine
- [ ] Tests avec mock Pinecone
- [ ] Tests de performance (latency <300ms)
- [ ] Tests de pertinence (relevance >0.8)
- [ ] Integration avec existing RAGEngine

**Tests critiques:**
```typescript
describe('DualQueryRetrievalEngine', () => {
  it('should retrieve from multiple sources', async () => {
    const results = await engine.retrieve(testEmbedding, 'methodology', 'company-a', {});

    expect(results.metadata.supportCount).toBeGreaterThan(0);
    expect(results.metadata.historicalCount).toBeGreaterThan(0);
  });

  it('should deduplicate chunks', async () => {
    const results = await engine.retrieve(testEmbedding, 'methodology', 'company-a', {});

    const chunkKeys = results.chunks.map(c =>
      `${c.metadata.documentId}:${c.metadata.chunkIndex}`
    );
    const uniqueKeys = new Set(chunkKeys);

    expect(chunkKeys.length).toBe(uniqueKeys.size);
  });

  it('should respect topK limit', async () => {
    const results = await engine.retrieve(testEmbedding, 'methodology', 'company-a', {
      depth: 'basic' // topK = 5
    });

    expect(results.chunks.length).toBeLessThanOrEqual(5);
  });

  it('should boost pinned sources', async () => {
    const results = await engine.retrieve(testEmbedding, 'methodology', 'company-a', {
      pinnedSourceRfpId: 'rfp-123'
    });

    const pinnedChunks = results.chunks.filter(c => c.source === 'pinned');
    const avgPinnedScore = pinnedChunks.reduce((sum, c) => sum + c.compositeScore, 0) / pinnedChunks.length;
    const avgOtherScore = results.chunks
      .filter(c => c.source !== 'pinned')
      .reduce((sum, c) => sum + c.compositeScore, 0) / (results.chunks.length - pinnedChunks.length);

    expect(avgPinnedScore).toBeGreaterThan(avgOtherScore);
  });
});
```

---

## üìã Phase 2: Frontend UI (7 jours)

### Jour 8-9: Unified Upload Wizard

**Objectif:** Wizard 5 √©tapes accessible et mobile-ready

```tsx
// src/components/knowledge-base/unified-upload-wizard.tsx
'use client';

import { useState } from 'react';
import { Dialog, DialogContent, DialogHeader, DialogTitle } from '@/components/ui/dialog';
import { Progress } from '@/components/ui/progress';
import { useToast } from '@/hooks/use-toast';

const WIZARD_STEPS = [
  { id: 'upload', label: 'Upload', duration: '~30s' },
  { id: 'analysis', label: 'Analyse IA', duration: '~10s' },
  { id: 'validation', label: 'Validation', duration: '~1min' },
  { id: 'processing', label: 'Traitement RAG', duration: '~8s' },
  { id: 'confirmation', label: 'Termin√©', duration: '' }
];

export function UnifiedUploadWizard({ open, onOpenChange }: Props) {
  const [currentStep, setCurrentStep] = useState(0);
  const [file, setFile] = useState<File | null>(null);
  const [analysis, setAnalysis] = useState<DocumentAnalysis | null>(null);
  const { toast } = useToast();

  async function handleUpload(selectedFile: File) {
    setFile(selectedFile);
    setCurrentStep(1); // Analysis

    try {
      // Upload + trigger analysis
      const formData = new FormData();
      formData.append('file', selectedFile);

      const response = await fetch('/api/knowledge-base/upload', {
        method: 'POST',
        body: formData
      });

      const { documentId } = await response.json();

      // Poll for analysis results
      const result = await pollAnalysisResult(documentId);
      setAnalysis(result);
      setCurrentStep(2); // Validation
    } catch (error) {
      toast({
        title: 'Erreur',
        description: error.message,
        variant: 'destructive'
      });
    }
  }

  return (
    <Dialog open={open} onOpenChange={onOpenChange}>
      <DialogContent className="max-w-2xl" aria-labelledby="wizard-title">
        <DialogHeader>
          <DialogTitle id="wizard-title">Ajouter un Document</DialogTitle>
        </DialogHeader>

        {/* Progress indicator */}
        <nav aria-label="Progression du wizard">
          <ol className="flex items-center gap-2">
            {WIZARD_STEPS.map((step, index) => (
              <li key={step.id} className="flex items-center gap-2">
                <StepIndicator
                  step={step}
                  current={currentStep === index}
                  completed={currentStep > index}
                  aria-current={currentStep === index ? 'step' : undefined}
                />
              </li>
            ))}
          </ol>
        </nav>

        {/* Step content */}
        <div className="mt-6">
          {currentStep === 0 && <UploadStep onUpload={handleUpload} />}
          {currentStep === 1 && <AnalysisStep />}
          {currentStep === 2 && <ValidationStep analysis={analysis} onNext={() => setCurrentStep(3)} />}
          {currentStep === 3 && <ProcessingStep />}
          {currentStep === 4 && <ConfirmationStep />}
        </div>

        {/* Keyboard shortcuts */}
        <div className="mt-4 border-t pt-4">
          <p className="text-xs text-muted-foreground">
            Raccourcis: <kbd className="px-2 py-1 bg-muted rounded">‚åò Enter</kbd> Valider ‚Ä¢{' '}
            <kbd className="px-2 py-1 bg-muted rounded">Esc</kbd> Annuler
          </p>
        </div>

        {/* Screen reader announcements */}
        <div role="status" aria-live="polite" aria-atomic="true" className="sr-only">
          {getStepAnnouncement(currentStep, analysis)}
        </div>
      </DialogContent>
    </Dialog>
  );
}

function getStepAnnouncement(step: number, analysis: DocumentAnalysis | null): string {
  const messages = [
    '√âtape 1 sur 5: Upload. S√©lectionnez un fichier PDF, DOCX ou TXT.',
    '√âtape 2 sur 5: Analyse en cours. Veuillez patienter.',
    analysis ? `Analyse termin√©e. Type d√©tect√©: ${analysis.documentType}. ${analysis.suggestedCategories.length} cat√©gories sugg√©r√©es.` : '',
    '√âtape 3 sur 5: Validation. V√©rifiez les m√©tadonn√©es d√©tect√©es.',
    '√âtape 4 sur 5: Traitement RAG en cours.',
    '√âtape 5 sur 5: Document ajout√© avec succ√®s.'
  ];
  return messages[step] || '';
}
```

**T√¢ches:**
- [ ] Impl√©menter wizard 5 √©tapes
- [ ] Accessibility WCAG 2.1 AA (keyboard nav, screen reader, contrast)
- [ ] Mobile responsive (bottom sheet)
- [ ] Progressive disclosure
- [ ] Tests E2E (Playwright)

---

### Jour 10-11: Analytics Dashboard

**Objectif:** Dashboard avec insights actionnables

```tsx
// src/app/(dashboard)/knowledge-base/analytics/page.tsx
import { InsightsPanel } from '@/components/knowledge-base/insights-panel';
import { UsageMetrics } from '@/components/knowledge-base/usage-metrics';
import { DocumentList } from '@/components/knowledge-base/document-list';

export default async function KnowledgeBaseAnalyticsPage() {
  const session = await getServerSession();
  const companyId = session!.user.companyId;

  // Fetch data
  const [documents, usage, benchmarks] = await Promise.all([
    getDocuments(companyId),
    getUsageData(companyId),
    getBenchmarkData()
  ]);

  const insights = generateActionableInsights(documents, usage, benchmarks);

  return (
    <div className="space-y-6">
      <h1 className="text-3xl font-bold">Biblioth√®que de Connaissances</h1>

      {/* Actionable Insights */}
      <InsightsPanel insights={insights} />

      {/* Key Metrics */}
      <div className="grid grid-cols-1 md:grid-cols-3 gap-4">
        <MetricCard
          title="Documents de Support"
          value={documents.filter(d => d.documentPurpose === 'rfp_support').length}
          change="+12 ce mois"
          trend="up"
        />
        <MetricCard
          title="Taux d'Utilisation"
          value={`${Math.round(usage.avgUsageRate * 100)}%`}
          change="vs 68% moyenne"
          trend={usage.avgUsageRate > 0.68 ? 'up' : 'down'}
        />
        <MetricCard
          title="Couverture Cat√©gories"
          value={`${usage.categoriesCovered} / ${usage.totalCategories}`}
          change={`${Math.round(usage.categoriesCovered / usage.totalCategories * 100)}%`}
        />
      </div>

      {/* Usage over time */}
      <UsageMetrics data={usage.overTime} />

      {/* Document list with actions */}
      <DocumentList documents={documents} usage={usage} />
    </div>
  );
}
```

**Insights Engine:**
```typescript
// src/lib/analytics/insights-engine.ts
export function generateActionableInsights(
  docs: Document[],
  usage: UsageData,
  benchmarks: BenchmarkData
): Insight[] {
  const insights: Insight[] = [];

  // Insight 1: Coverage gaps
  const categoryFrequency = usage.rfpCategoryFrequency;
  const categoryDocCount = usage.categoryDocumentCount;

  Object.entries(categoryFrequency).forEach(([cat, freq]) => {
    const docCount = categoryDocCount[cat] || 0;
    const coverageRatio = docCount / freq;

    if (coverageRatio < 0.5 && freq > 5) {
      insights.push({
        type: 'alert',
        title: `Couverture faible : ${CATEGORY_LABELS[cat]}`,
        description: `Cette cat√©gorie appara√Æt dans ${freq} RFPs mais vous n'avez que ${docCount} document(s).`,
        recommendation: 'Ajoutez des documents pour am√©liorer les r√©ponses.',
        actions: [
          {
            label: 'Uploader un document',
            href: `/knowledge-base/upload?suggestedCategory=${cat}`,
            primary: true
          }
        ]
      });
    }
  });

  // Insight 2: Unused documents
  const unusedDocs = docs.filter(
    d => usage.documentUsageCount[d.id] === 0 && daysSince(d.createdAt) > 90
  );

  if (unusedDocs.length > 0) {
    insights.push({
      type: 'warning',
      title: 'Documents inutilis√©s',
      description: `${unusedDocs.length} documents n'ont jamais √©t√© utilis√©s depuis 90 jours.`,
      recommendation: 'Archivez les documents obsol√®tes pour am√©liorer la pertinence.',
      actions: [
        { label: 'Voir la liste', href: '/knowledge-base?filter=unused' },
        { label: 'Archiver en masse', onClick: () => bulkArchive(unusedDocs.map(d => d.id)) }
      ]
    });
  }

  // Insight 3: High performers
  const topDocs = Object.entries(usage.documentUsageCount)
    .sort((a, b) => b[1] - a[1])
    .slice(0, 3)
    .map(([docId]) => docs.find(d => d.id === docId)!);

  if (topDocs.length > 0) {
    insights.push({
      type: 'opportunity',
      title: 'Documents les plus utilis√©s',
      description: `Ces documents sont utilis√©s dans ${usage.documentUsageCount[topDocs[0].id]} r√©ponses.`,
      recommendation: 'Cr√©ez plus de contenus similaires.',
      actions: [
        { label: 'Voir les d√©tails', href: `/knowledge-base/${topDocs[0].id}` }
      ]
    });
  }

  return insights;
}
```

**T√¢ches:**
- [ ] Impl√©menter insights engine
- [ ] Dashboard analytics
- [ ] Usage tracking (PostHog events)
- [ ] Benchmarks (fake data for MVP)
- [ ] Tests visuels

---

## üìã Phase 3: Testing & QA (4 jours)

### Jour 12-13: Tests Automatis√©s

#### Unit Tests (Coverage >80%)

```typescript
// tests/unit/dual-query-engine.test.ts
describe('DualQueryRetrievalEngine', () => {
  // ... (tests d√©j√† d√©finis plus haut)
});

// tests/unit/document-analysis.test.ts
describe('Document Analysis Service', () => {
  it('should categorize methodology guides correctly', async () => {
    const text = fs.readFileSync('fixtures/agile-guide.pdf', 'utf-8');
    const result = await analyzeDocument(text, 'agile-guide.pdf');

    expect(result.documentType).toBe('methodology_guide');
    expect(result.confidence).toBeGreaterThan(0.8);
    expect(result.suggestedCategories).toContainEqual({
      category: 'project-methodology',
      confidence: expect.any(Number)
    });
  });

  it('should handle PDFs with mixed content', async () => {
    const text = fs.readFileSync('fixtures/mixed-content.pdf', 'utf-8');
    const result = await analyzeDocument(text, 'mixed.pdf');

    expect(result.documentType).toBeDefined();
    expect(result.suggestedCategories.length).toBeGreaterThan(0);
  });

  it('should retry with Sonnet if Haiku confidence <0.7', async () => {
    // Mock low confidence from Haiku
    vi.spyOn(anthropic.messages, 'create')
      .mockResolvedValueOnce({ content: [{ type: 'text', text: JSON.stringify({ confidence: 0.6 }) }] })
      .mockResolvedValueOnce({ content: [{ type: 'text', text: JSON.stringify({ confidence: 0.95 }) }] });

    const result = await analyzeDocument('ambiguous text', 'test.pdf');

    expect(result.confidence).toBeGreaterThan(0.9);
  });
});
```

#### Integration Tests

```typescript
// tests/integration/upload-workflow.test.ts
describe('Upload Workflow E2E', () => {
  it('should upload, analyze, and embed a support document', async () => {
    const file = new File(['test content'], 'test.pdf', { type: 'application/pdf' });

    // 1. Upload
    const formData = new FormData();
    formData.append('file', file);

    const uploadRes = await fetch('/api/knowledge-base/upload', {
      method: 'POST',
      body: formData,
      headers: { Cookie: authCookie }
    });

    expect(uploadRes.status).toBe(200);
    const { documentId } = await uploadRes.json();

    // 2. Wait for analysis
    await waitFor(async () => {
      const doc = await db.query.documents.findFirst({
        where: eq(documents.id, documentId)
      });
      expect(doc?.metadata?.analysisComplete).toBe(true);
    }, { timeout: 30000 });

    // 3. Verify Pinecone vectors
    const namespace = pineconeIndex.namespace('rfp-library-production');
    const stats = await namespace.describeIndexStats();

    const vectorsAfter = stats.namespaces['rfp-library-production'].vectorCount;
    expect(vectorsAfter).toBeGreaterThan(0);
  });
});
```

#### Performance Tests

```typescript
// tests/performance/retrieval-latency.test.ts
describe('Retrieval Performance', () => {
  it('should retrieve results in <300ms (P95)', async () => {
    const latencies: number[] = [];

    for (let i = 0; i < 100; i++) {
      const start = Date.now();
      await engine.retrieve(testEmbedding, 'methodology', 'company-a', {});
      const latency = Date.now() - start;
      latencies.push(latency);
    }

    latencies.sort((a, b) => a - b);
    const p95 = latencies[Math.floor(latencies.length * 0.95)];

    console.log(`P50: ${latencies[50]}ms, P95: ${p95}ms, P99: ${latencies[99]}ms`);
    expect(p95).toBeLessThan(300);
  });
});
```

**T√¢ches:**
- [ ] Unit tests (>80% coverage)
- [ ] Integration tests (happy path + edge cases)
- [ ] Performance tests (latency benchmarks)
- [ ] Security tests (multi-tenant isolation)
- [ ] Load tests (100 concurrent uploads)

---

### Jour 14-15: UAT & Bug Fixes

**User Acceptance Testing avec 5 utilisateurs internes**

#### Test Plan

1. **Upload Workflow** (30 min)
   - Uploader 5 documents diff√©rents (PDF, DOCX)
   - Valider auto-cat√©gorisation
   - Corriger tags si n√©cessaire
   - Mesurer temps total

2. **Retrieval Quality** (30 min)
   - G√©n√©rer 10 r√©ponses RFP
   - Noter pertinence des sources (1-5)
   - V√©rifier pr√©sence docs support
   - Comparer avec baseline (sans support docs)

3. **Analytics** (15 min)
   - Explorer dashboard
   - Comprendre insights
   - Tester actions (archiver, uploader)

4. **Mobile** (15 min)
   - Tester wizard sur mobile
   - Upload via cam√©ra
   - Validation tags tactile

#### Feedback Collection

```typescript
// Questionnaire post-UAT
const UATFeedback = {
  usability: {
    uploadWizardClarity: 1-5,
    analysisWaitTime: 1-5,
    validationStepUtility: 1-5,
  },
  performance: {
    uploadSpeed: 1-5,
    retrievalSpeed: 1-5,
  },
  value: {
    relevanceImprovement: 1-5,
    timeSaved: 'hours/week',
    willingToAdopt: boolean,
  },
  bugs: Array<{
    severity: 'critical' | 'major' | 'minor',
    description: string,
    steps: string[],
  }>
};
```

**T√¢ches:**
- [ ] Recruter 5 beta testers
- [ ] Conduire UAT sessions (enregistr√©es)
- [ ] Collecter feedback
- [ ] Triage bugs (P0/P1/P2)
- [ ] Fixer bugs critiques
- [ ] It√©rer si feedback <4/5

---

## üìã Phase 4: Deployment (2 jours)

### Jour 16: Staging Deployment

```bash
# 1. Backup production DB
pg_dump -h prod-db.amazonaws.com -U admin -d market_intelligence > backup_$(date +%Y%m%d).sql

# 2. Deploy to staging
git checkout feature/support-docs-rag
git pull origin main
npm run build
vercel deploy --env=staging

# 3. Run migrations on staging
export DATABASE_URL=postgresql://staging-db...
npm run db:migrate

# 4. Smoke tests
npm run test:smoke -- --env=staging

# 5. Manual QA
# - Upload 10 documents
# - Generate 20 responses
# - Check analytics
# - Verify no errors in Datadog
```

**T√¢ches:**
- [ ] Backup production DB
- [ ] Deploy to staging
- [ ] Run migrations
- [ ] Smoke tests
- [ ] Manual QA checklist
- [ ] Performance benchmarks
- [ ] Security scan

---

### Jour 17: Production Deployment

#### Feature Flag Strategy

```typescript
// Feature flag configuration (LaunchDarkly / Vercel Flags)
const SUPPORT_DOCS_ROLLOUT = {
  phase1: { percentage: 10, duration: '3 days' },  // Soft launch
  phase2: { percentage: 50, duration: '2 days' },  // Half rollout
  phase3: { percentage: 100, duration: 'forever' } // Full launch
};

// Usage in code
export async function generateResponse(questionId: string) {
  const isSupportDocsEnabled = await getFeatureFlag('support-docs-enabled');

  if (isSupportDocsEnabled) {
    // Use DualQueryRetrievalEngine
    return dualQueryEngine.retrieve(...);
  } else {
    // Fallback to old single-query
    return legacyEngine.retrieve(...);
  }
}
```

#### Deployment Checklist

- [ ] Feature flag cr√©√© (`support-docs-enabled` = false)
- [ ] Monitoring configur√© (Datadog dashboards)
- [ ] Alerts configur√©s (error rate >1%, latency P95 >800ms)
- [ ] Runbook cr√©√© (rollback procedures)
- [ ] Deploy production
- [ ] Run migrations (with backup)
- [ ] Smoke tests production
- [ ] Enable feature flag 10% (internal users)
- [ ] Monitor 24h
- [ ] If stable ‚Üí 50% ‚Üí 24h ‚Üí 100%

#### Rollback Plan

```bash
# If critical issues detected:
# 1. Disable feature flag immediately
vercel env add SUPPORT_DOCS_ENABLED=false

# 2. Rollback deployment
vercel rollback

# 3. Rollback migrations if needed
npm run db:rollback

# 4. Notify stakeholders
slack-cli send --channel #incidents "Support docs rolled back due to..."
```

**Monitoring Dashboards:**
- Upload success rate (target: >99%)
- Analysis latency P95 (target: <15s)
- Retrieval latency P95 (target: <300ms)
- Error rate (target: <0.1%)
- API costs per day (alert if >$50/day)

---

## üí∞ Budget Consolid√©

### Co√ªts API (100 Documents + 1000 Questions)

| Service | Volume | Co√ªt Unitaire | Total |
|---------|--------|---------------|-------|
| **Claude Haiku** (Analysis) | 100 docs √ó 20K tokens | $0.00025/1K | $0.50 |
| **Claude Sonnet** (Retry 30%) | 30 docs √ó 20K tokens | $0.003/1K | $1.80 |
| **Claude Sonnet** (Preprocessing) | 100 docs √ó 300K tokens | $0.003/1K | $30.00 |
| **OpenAI Embeddings** (small) | 100 docs √ó 500K tokens | $0.00002/1K | $0.01 |
| **OpenAI Embeddings** (queries) | 1000 queries √ó 100 tokens | $0.00002/1K | $0.002 |
| **Claude Sonnet** (Generation) | 1000 responses √ó 5K tokens | $0.015/1K | $15.00 |
| **Pinecone** (storage) | 25K vectors | $0.0004/1K/month | $0.60 |
| **S3** (storage) | 100 docs √ó 2MB | $0.023/GB | $0.005 |
| **TOTAL** | | | **$47.91** |

**Note:** Co√ªt r√©el ~$48/100 docs (vs $7-15 estim√© initialement)

### Co√ªts D√©veloppement (24 jours)

| R√¥le | Taux/jour | Jours | Total |
|------|-----------|-------|-------|
| Backend Engineer | ‚Ç¨400 | 11j | ‚Ç¨4,400 |
| Frontend Engineer | ‚Ç¨400 | 9j | ‚Ç¨3,600 |
| QA Engineer | ‚Ç¨300 | 4j | ‚Ç¨1,200 |
| Product Manager | ‚Ç¨350 | 3j | ‚Ç¨1,050 |
| **TOTAL** | | **24j** | **‚Ç¨10,250** |

### Infrastructure Mensuelle

| Service | Co√ªt |
|---------|------|
| Vercel Pro | $20 |
| PostgreSQL (Supabase) | $25 |
| Pinecone Serverless | $70 |
| S3 + CloudFront | $30 |
| Monitoring (Datadog) | $50 |
| **TOTAL** | **$195/mois** |

### Budget Total Ann√©e 1

| Cat√©gorie | Montant |
|-----------|---------|
| D√©veloppement (one-time) | ‚Ç¨10,250 |
| API costs (1000 docs/an) | ‚Ç¨700 |
| Infrastructure (12 mois) | ‚Ç¨2,100 |
| Contingency (15%) | ‚Ç¨1,958 |
| **TOTAL** | **‚Ç¨15,008** |

---

## üéØ M√©triques de Succ√®s

### Adoption (Objectif: >60% √† 3 mois)

```typescript
interface AdoptionMetrics {
  newFeatureViewRate: number;      // >90%
  firstUploadWithin7Days: number;  // >50%
  activeUsersPerWeek: number;      // >30
  avgDocsPerUser: number;          // >20
  avgReusesPerDoc: number;         // >5
}
```

### Qualit√© RAG (Objectif: +30% am√©lioration)

```typescript
interface RAGQualityMetrics {
  avgRelevanceScore: number;       // >0.8 (vs 0.6 baseline)
  userAcceptanceRate: number;      // >80% (vs 65% baseline)
  supportDocsUsageRate: number;    // >70% des docs utilis√©s ‚â•1 fois
  avgSourcesPerResponse: number;   // >3
  userRating: number;              // >4.2/5
}
```

### Performance Technique

```typescript
interface PerformanceMetrics {
  p50AnalysisDuration: number;     // <8s
  p95AnalysisDuration: number;     // <15s
  p95RetrievalLatency: number;     // <300ms
  uploadSuccessRate: number;       // >99%
  errorRate: number;               // <0.1%
}
```

---

## üö® Risques & Mitigation

### Risques Techniques

| Risque | Probabilit√© | Impact | Mitigation |
|--------|-------------|--------|------------|
| **Pinecone latency >300ms √† scale** | Moyenne | √âlev√© | - Benchmark avec 50K vectors<br>- Namespace per company si >100 companies<br>- Caching agressif |
| **Claude API timeout >30s** | √âlev√©e | Moyen | - Streaming responses (SSE)<br>- Background processing<br>- Fallback to Haiku |
| **Multi-tenant data leak** | Faible | Critique | - Security tests exhaustifs<br>- RLS PostgreSQL<br>- External security audit |
| **Embedding costs explosent** | Moyenne | Moyen | - Batch processing strict<br>- Daily cost monitoring<br>- Alert si >$50/day |

### Risques UX

| Risque | Probabilit√© | Impact | Mitigation |
|--------|-------------|--------|------------|
| **Adoption <20%** | Moyenne | √âlev√© | - Onboarding obligatoire<br>- In-app tooltips<br>- Email campaign |
| **Wizard trop complexe** | Faible | Moyen | - UAT avec 5 users<br>- A/B test simplified flow<br>- Skip option for experts |
| **Mobile frustrant** | Moyenne | Moyen | - Mobile-first design<br>- Touch targets ‚â•44px<br>- Camera scan feature |

### Risques Business

| Risque | Probabilit√© | Impact | Mitigation |
|--------|-------------|--------|------------|
| **Timeline >28 jours** | Moyenne | Moyen | - Buffer 4 jours inclus<br>- Daily standups<br>- Scope reduction si needed |
| **Budget >‚Ç¨13K** | Faible | Faible | - Weekly cost tracking<br>- Contingency ‚Ç¨1,958 |

---

## ‚úÖ Checklist de Lancement

### Pr√©-lancement

- [ ] Toutes les migrations DB appliqu√©es en staging
- [ ] Tests automatis√©s passent (coverage >80%)
- [ ] UAT compl√©t√© avec ‚â•4/5 satisfaction
- [ ] Accessibility audit WCAG 2.1 AA
- [ ] Security review (multi-tenant isolation)
- [ ] Performance benchmarks valid√©s
- [ ] Documentation compl√®te
- [ ] Monitoring configur√©
- [ ] Feature flag cr√©√©
- [ ] Runbook cr√©√©

### Soft Launch (10% users, 3 jours)

- [ ] 10% rollout via feature flag
- [ ] Monitoring actif 24/7
- [ ] Daily metrics review
- [ ] No critical bugs
- [ ] Latency P95 <800ms
- [ ] Error rate <0.5%

### Full Launch (100%)

- [ ] Soft launch stable 3 jours
- [ ] User feedback >4/5
- [ ] Feature flag ‚Üí 50% (2 jours)
- [ ] Feature flag ‚Üí 100%
- [ ] Announcement (email + in-app)
- [ ] Blog post (optional)

---

## üöÄ Prochaines √âtapes Imm√©diates

### Cette Semaine

1. **Approval stakeholders:**
   - [ ] Budget ‚Ç¨15K approuv√©
   - [ ] Timeline 24j acceptable
   - [ ] Assigner √©quipe (backend, frontend, QA, PM)

2. **D√©cisions techniques:**
   - [ ] Pinecone: Namespace unique ou s√©par√©?
   - [ ] Embedding: text-embedding-3-small ou large?
   - [ ] Analytics: Dashboard complet ou MVP?

3. **Setup projet:**
   - [ ] Cr√©er feature branch `feature/support-docs-rag-v4`
   - [ ] Setup feature flag (LaunchDarkly)
   - [ ] Cr√©er Jira epic + 24 stories
   - [ ] Kickoff meeting √©quipe

### Semaine Prochaine

1. **D√©marrer Phase 0.5** (3 jours)
   - POC dual queries Pinecone
   - Migration base de donn√©es
   - Uniformisation tenant_id

2. **Environment prep:**
   - [ ] Backup production DB
   - [ ] Setup staging environment
   - [ ] Configure monitoring (Datadog)

---

## üìû Contacts

| R√¥le | Responsable |
|------|-------------|
| Product Owner | [√Ä d√©finir] |
| Tech Lead | [√Ä d√©finir] |
| Backend Engineer | [√Ä d√©finir] |
| Frontend Engineer | [√Ä d√©finir] |
| QA Engineer | [√Ä d√©finir] |

---

## üìù Version History

### v4.0 (Current - Production Ready)
- ‚úÖ Toutes corrections audit architecture appliqu√©es
- ‚úÖ Timeline r√©aliste 24 jours
- ‚úÖ Co√ªts r√©els $48/100 docs
- ‚úÖ Phase 0.5 ajout√©e (3j corrections critiques)
- ‚úÖ Dual queries Pinecone strategy compl√®te
- ‚úÖ Schema migration d√©taill√©e
- ‚úÖ Tests exhaustifs d√©finis

### v3.0 (Final apr√®s audits)
- Audit architecture + UX appliqu√©s
- Timeline 32j ‚Üí r√©vis√© √† 24j (scope optimis√©)

### v1.0 (Initial)
- ‚ùå Timeline irr√©aliste (15j)
- ‚ùå Co√ªts sous-estim√©s ($7-15)
- ‚ùå Pinecone filtering impossible

---

**FIN DU PLAN D'IMPL√âMENTATION R√âVIS√â v4.0**

**Statut:** ‚úÖ **Ready for Execution**
**Prochaine action:** Obtenir approbation stakeholders + d√©marrer Phase 0.5
**Success probability:** **85-90%** ‚úÖ
