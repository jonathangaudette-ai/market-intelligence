# TODO: Architecture Asynchrone pour Parsing RFP

## Contexte

Actuellement, le parsing RFP est **synchrone** : l'API route attend la fin du traitement complet avant de r√©pondre. Cela fonctionne avec `maxDuration = 300` sur Vercel Pro, mais pr√©sente des limitations :

- ‚è±Ô∏è Timeout de 5 minutes max (limite Vercel Pro)
- üîÑ Pas de possibilit√© de retry en cas d'√©chec partiel
- üìä Updates de progression d√©pendent du polling frontend
- üí∞ Co√ªt: chaque requ√™te garde une fonction serverless active pendant ~3 minutes

## Option 2: Architecture Asynchrone (Recommand√© pour Production)

### Architecture Propos√©e

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     POST /parse      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Frontend  ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ  API Route       ‚îÇ
‚îÇ             ‚îÇ<‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  (accepte & ret.) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     202 Accepted     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                                       ‚îÇ
       ‚îÇ Polling /progress                    ‚îÇ Enqueue job
       ‚îÇ                                       ‚ñº
       ‚îÇ                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ                              ‚îÇ  Queue/Worker    ‚îÇ
       ‚îÇ                              ‚îÇ  (Inngest/QStash)‚îÇ
       ‚îÇ                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                                       ‚îÇ
       ‚îÇ                                       ‚îÇ Process RFP
       ‚îÇ                                       ‚ñº
       ‚îÇ                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ                              ‚îÇ  Background Job  ‚îÇ
       ‚îÇ                              ‚îÇ  - Parse PDF     ‚îÇ
       ‚îÇ                              ‚îÇ  - Extract (GPT) ‚îÇ
       ‚îÇ                              ‚îÇ  - Categorize    ‚îÇ
       ‚îÇ                              ‚îÇ  - Save to DB    ‚îÇ
       ‚îÇ                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                                       ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                Updates DB progress
```

### Solutions Possibles

#### 1. **Vercel Cron + Vercel KV** (Simple, Gratuit)
- Cron job v√©rifie les RFPs "pending" toutes les minutes
- KV stocke le state de progression
- ‚úÖ Gratuit sur Hobby plan
- ‚úÖ Facile √† impl√©menter
- ‚ùå Moins scalable

#### 2. **Upstash QStash** (Recommand√©)
- Queue management avec retry automatique
- Webhooks pour notifier le frontend
- ‚úÖ Retry automatique en cas d'√©chec
- ‚úÖ Excellent pour production
- üí∞ ~$10/mois

#### 3. **Inngest** (Premium)
- Workflow orchestration avec visual monitoring
- Steps isol√©s (parse ‚Üí extract ‚Üí categorize)
- ‚úÖ Monitoring excellent
- ‚úÖ Retry granulaire par step
- üí∞ ~$20/mois (after free tier)

### Impl√©mentation Sugg√©r√©e (QStash)

#### √âtape 1: Installer QStash

```bash
npm install @upstash/qstash
```

#### √âtape 2: Modifier l'API Route

```typescript
// src/app/api/v1/rfp/rfps/[id]/parse/route.ts
export async function POST(request: NextRequest, { params }) {
  // ... auth & validation ...
  
  // Set to processing immediately
  await db.update(rfps)
    .set({ parsingStatus: 'processing', parsingStage: 'queued' })
    .where(eq(rfps.id, id));
  
  // Enqueue background job
  await qstashClient.publishJSON({
    url: `${process.env.NEXT_PUBLIC_URL}/api/v1/rfp/jobs/parse`,
    body: { rfpId: id, companyId: company.id },
  });
  
  // Return immediately
  return NextResponse.json({ 
    message: 'RFP parsing queued',
    rfpId: id,
    status: 'processing' 
  }, { status: 202 });
}
```

#### √âtape 3: Cr√©er le Worker

```typescript
// src/app/api/v1/rfp/jobs/parse/route.ts
export async function POST(request: NextRequest) {
  const { rfpId, companyId } = await request.json();
  
  try {
    // Parse document
    await db.update(rfps).set({ parsingStage: 'parsing' });
    const doc = await parseDocument(...);
    
    // Extract questions
    await db.update(rfps).set({ parsingStage: 'extracting' });
    const questions = await extractQuestionsInBatches(doc.text, {
      onProgress: async (current, total, found) => {
        await db.update(rfps).set({
          parsingProgressCurrent: current,
          parsingProgressTotal: total,
          questionsExtracted: found,
        });
      }
    });
    
    // Categorize
    await db.update(rfps).set({ parsingStage: 'categorizing' });
    // ... rest of logic ...
    
    return NextResponse.json({ success: true });
  } catch (error) {
    await db.update(rfps).set({ 
      parsingStatus: 'failed',
      parsingError: error.message 
    });
    throw error; // QStash will retry
  }
}
```

### Avantages Architecture Asynchrone

‚úÖ **Scalabilit√©**: Pas de timeout, peut traiter des RFPs de 1000+ pages  
‚úÖ **Retry**: Reprise automatique en cas d'√©chec temporaire (rate limit OpenAI)  
‚úÖ **Co√ªt**: Workers s'ex√©cutent uniquement quand n√©cessaire  
‚úÖ **UX**: R√©ponse instantan√©e au frontend (202 Accepted)  
‚úÖ **Monitoring**: Logs centralis√©s dans le dashboard QStash/Inngest  
‚úÖ **Parall√©lisation**: Peut traiter plusieurs RFPs en m√™me temps  

### Timeline Sugg√©r√©e

- **Phase 1** (Actuel): Architecture synchrone avec `maxDuration = 300` ‚úÖ
- **Phase 2** (Avant production): Impl√©menter QStash pour background processing
- **Phase 3** (Optimisation): Parall√©liser extraction + cat√©gorisation

### Ressources

- [QStash Documentation](https://upstash.com/docs/qstash)
- [Inngest Documentation](https://www.inngest.com/docs)
- [Vercel Cron Jobs](https://vercel.com/docs/cron-jobs)

---

**Priorit√©**: Medium (apr√®s MVP stable)  
**Effort**: 2-3 jours de d√©veloppement  
**ROI**: Haute (meilleure UX, plus scalable, moins de timeouts)
