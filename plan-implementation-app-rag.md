# Plan d'Impl√©mentation - Application RAG Intelligence Comp√©titive
## De la Vision au Code: Construisons Ensemble

**Date:** 1er novembre 2025
**Objectif:** Cr√©er une application fonctionnelle avec RAG pour l'intelligence comp√©titive
**Philosophie:** Start small, iterate fast, scale progressively

---

## 1. Qu'allons-nous Construire?

### 1.1 MVP - Application RAG Fonctionnelle (Semaine 1-2)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   COMPETITIVE INTEL APP v0.1                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

USER INTERFACE (Web)
  ‚îú‚îÄ Chat conversationnel (interface principale)
  ‚îú‚îÄ Upload de documents (PDFs, sites web)
  ‚îî‚îÄ Affichage des sources cit√©es

BACKEND (FastAPI)
  ‚îú‚îÄ API REST endpoints
  ‚îú‚îÄ RAG Engine (Claude Sonnet 4.5 + Pinecone)
  ‚îú‚îÄ Document processing pipeline
  ‚îî‚îÄ MCP integrations (Firecrawl, Brave Search)

DATA LAYER
  ‚îú‚îÄ Pinecone (vector store)
  ‚îú‚îÄ PostgreSQL (metadata, chat history)
  ‚îî‚îÄ S3 / Local storage (raw documents)

CORE FEATURES
  ‚úì Upload PDF (rapport concurrent, case study)
  ‚úì Crawl website concurrent (via Firecrawl)
  ‚úì Chat avec contexte RAG
  ‚úì Citations avec sources
  ‚úì M√©moire de conversation
  ‚úì Multi-document synthesis
```

### 1.2 Ce que vous pourrez faire (MVP)

```
Example 1: Analyser un concurrent
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
YOU: "Upload rapport-acme-corp.pdf"
APP: ‚úì Document uploaded (47 pages, 12,000 tokens)
     ‚úì Chunked into 58 segments
     ‚úì Embedded and indexed in Pinecone

YOU: "Quelles sont les principales forces d'Acme Corp?"
APP: [Analyse le PDF via RAG, r√©pond avec citations]
     "D'apr√®s le rapport (page 12), les principales forces sont:
     1. Strong brand recognition (mentionn√© 8 fois)
     2. Enterprise customer base (45% de leur revenue)
     3. Technical expertise in AI (15 ML engineers hired Q3)

     Sources: rapport-acme-corp.pdf (p.12, p.23, p.31)"

YOU: "Compare leur pricing au n√¥tre"
APP: [Se souvient du contexte, continue la conversation]
     "Le rapport indique leur pricing √† $899/mo pour Enterprise
     (page 18). Si je compare √† votre pricing actuel...

     [Peut je crawler votre site de pricing pour comparaison?]"

YOU: "Oui, crawl https://ourcompany.com/pricing"
APP: ‚úì Crawling with Firecrawl...
     ‚úì Extracted pricing: $799/mo Enterprise

     "Comparaison:
     ‚Ä¢ Vous: $799/mo (100 cheaper)
     ‚Ä¢ Acme: $899/mo
     ‚Ä¢ Votre avantage: 11% moins cher

     Recommandation: Emphasize price advantage in battlecard"
```

```
Example 2: Recherche web augment√©e
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
YOU: "Trouve les derni√®res news sur Acme Corp"
APP: [Utilise Brave Search MCP]
     ‚úì Found 12 recent articles
     ‚úì Analyzing with Claude...

     "R√©sum√© des news (last 30 days):

     1. üéØ Series B Funding - $45M (Oct 15)
        Source: TechCrunch
        Implication: Expect aggressive hiring & expansion

     2. üíº New Enterprise Tier Launch (Oct 28)
        Source: Company blog
        Pricing: $899/mo (‚Üë from $799 previous)

     3. ü§ù Partnership with BigCo (Oct 20)
        Source: Press release
        Impact: Access to 1,500 BigCo customers

     [View all sources] [Add to knowledge base]"

YOU: "Ajoute ces news √† ma knowledge base"
APP: ‚úì 12 articles embedded in Pinecone
     ‚úì Available for future queries

     "Knowledge base updated. Vous pouvez maintenant demander:
     ‚Ä¢ 'Analyse l'impact de leur funding sur nous'
     ‚Ä¢ 'Compare leur partnership √† nos partenaires'
     ‚Ä¢ etc."
```

---

## 2. Architecture Technique

### 2.1 Stack Choisi (Simple mais Scalable)

```yaml
FRONTEND:
  framework: Next.js 14 (App Router)
  ui: Tailwind CSS + shadcn/ui
  chat: Vercel AI SDK (streaming)

BACKEND:
  framework: FastAPI (Python 3.11+)
  async: asyncio + httpx

AI/LLM:
  primary: Claude Sonnet 4.5 (Anthropic API)
  embeddings: text-embedding-3-large (OpenAI)

VECTOR DB:
  pinecone: Serverless (pay-per-use, easy setup)

DATABASE:
  postgresql: Supabase (managed, free tier)

STORAGE:
  files: Supabase Storage (ou S3 si pr√©f√©r√©)

INTEGRATIONS (MCP):
  - Firecrawl (web crawling)
  - Brave Search (recherche web)
  - (Extensible: Apify, etc.)

DEPLOYMENT:
  backend: Railway / Render (free tier disponible)
  frontend: Vercel (free tier)

DEV TOOLS:
  package_manager: Poetry (Python), pnpm (JS)
  code_quality: ruff, mypy, eslint
  env: docker-compose (dev local)
```

### 2.2 Pourquoi ce Stack?

| Choix | Raison |
|-------|--------|
| **FastAPI** | Async native, excellent pour RAG (I/O bound), auto-docs |
| **Claude Sonnet 4.5** | 200K context, extended thinking, meilleur raisonnement |
| **Pinecone Serverless** | Zero ops, pay-per-use, fast setup |
| **Supabase** | PostgreSQL manag√©, auth built-in, free tier g√©n√©reux |
| **Next.js 14** | SSR, streaming, excellent DX |
| **Vercel AI SDK** | Streaming chat out-of-the-box |

**Co√ªts estim√©s (MVP, 1-10 users):**
- Pinecone: $0-20/mois (serverless)
- Supabase: $0 (free tier suffit)
- Claude API: ~$50-200/mois (selon usage)
- Hosting: $0 (Vercel + Railway free tiers)
- **Total: $50-220/mois** pour d√©marrer

---

## 3. Structure du Projet

```
market-intelligence-app/
‚îú‚îÄ‚îÄ backend/                    # FastAPI backend
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py            # FastAPI app entry
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Configuration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/            # Pydantic models
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ document.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ competitor.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/          # Business logic
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rag_engine.py      # Core RAG logic
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embedding.py       # Embeddings
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ document_processor.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mcp_client.py      # MCP integrations
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ claude_client.py   # Claude API wrapper
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/               # API routes
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ documents.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ competitors.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ db/                # Database
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ postgres.py
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ pinecone.py
‚îÇ   ‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ pyproject.toml         # Poetry dependencies
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile
‚îÇ
‚îú‚îÄ‚îÄ frontend/                   # Next.js frontend
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ page.tsx           # Home / Chat interface
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ layout.tsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api/               # API routes (proxy)
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ChatInterface.tsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MessageList.tsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ChatInput.tsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ documents/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ DocumentUpload.tsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ui/                # shadcn components
‚îÇ   ‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api-client.ts      # Backend API client
‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îî‚îÄ‚îÄ next.config.js
‚îÇ
‚îú‚îÄ‚îÄ docker-compose.yml         # Dev environment
‚îú‚îÄ‚îÄ .env.example              # Environment variables template
‚îî‚îÄ‚îÄ README.md                 # Setup instructions
```

---

## 4. Plan d'Impl√©mentation (Progressive)

### Phase 1: Setup & Foundation (Jour 1-2)

```bash
‚úì T√¢ches
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚ñ° Cr√©er structure projet (backend + frontend)
‚ñ° Setup Poetry (backend dependencies)
‚ñ° Setup Next.js (frontend)
‚ñ° Configuration .env (API keys)
‚ñ° Docker compose (PostgreSQL local)
‚ñ° Test connections (Pinecone, Claude, PostgreSQL)

R√©sultat: Projet pr√™t, connexions valid√©es
```

### Phase 2: RAG Core Engine (Jour 3-5)

```bash
‚úì T√¢ches
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚ñ° Implement embedding service (OpenAI)
‚ñ° Implement Pinecone client (upsert, query)
‚ñ° Implement document processor (PDF ‚Üí chunks)
‚ñ° Implement RAG engine (retrieve + synthesize)
‚ñ° Test RAG pipeline end-to-end

R√©sultat: RAG engine fonctionnel (sans UI)
```

### Phase 3: API Backend (Jour 6-7)

```bash
‚úì T√¢ches
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚ñ° POST /api/documents/upload (upload PDF)
‚ñ° POST /api/documents/crawl (crawl website)
‚ñ° POST /api/chat (chat avec RAG)
‚ñ° GET /api/chat/history (historique)
‚ñ° PostgreSQL schema (conversations, documents)

R√©sultat: API REST compl√®te et test√©e
```

### Phase 4: Frontend Chat (Jour 8-10)

```bash
‚úì T√¢ches
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚ñ° Chat interface (composant React)
‚ñ° Message streaming (Vercel AI SDK)
‚ñ° Document upload UI
‚ñ° Citations display (sources avec liens)
‚ñ° Conversation history

R√©sultat: Interface utilisable, belle, responsive
```

### Phase 5: MCP Integrations (Jour 11-12)

```bash
‚úì T√¢ches
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚ñ° Firecrawl integration (crawl websites)
‚ñ° Brave Search integration (recherche web)
‚ñ° Tool calling avec Claude (dynamic MCP use)
‚ñ° Test int√©grations end-to-end

R√©sultat: Agent peut crawler web et chercher info
```

### Phase 6: Polish & Deploy (Jour 13-14)

```bash
‚úì T√¢ches
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚ñ° Error handling & validation
‚ñ° Loading states & feedback
‚ñ° Deploy backend (Railway/Render)
‚ñ° Deploy frontend (Vercel)
‚ñ° Documentation README

R√©sultat: App d√©ploy√©e, utilisable en production
```

---

## 5. Exemples de Code (Aper√ßu)

### 5.1 RAG Engine (Core Logic)

```python
# backend/app/services/rag_engine.py

from anthropic import AsyncAnthropic
from openai import AsyncOpenAI
import pinecone

class RAGEngine:
    def __init__(self):
        self.claude = AsyncAnthropic()
        self.openai = AsyncOpenAI()
        self.pinecone = pinecone.Index("intelligence")

    async def embed(self, text: str) -> list[float]:
        """Create embedding"""
        response = await self.openai.embeddings.create(
            model="text-embedding-3-large",
            input=text
        )
        return response.data[0].embedding

    async def retrieve(
        self,
        query: str,
        top_k: int = 5,
        filter: dict = None
    ) -> list[dict]:
        """Retrieve relevant documents from Pinecone"""

        # Embed query
        query_embedding = await self.embed(query)

        # Vector search
        results = self.pinecone.query(
            vector=query_embedding,
            top_k=top_k,
            filter=filter,
            include_metadata=True
        )

        return [
            {
                "text": match.metadata["text"],
                "source": match.metadata["source"],
                "page": match.metadata.get("page"),
                "score": match.score
            }
            for match in results.matches
        ]

    async def synthesize(
        self,
        query: str,
        context_docs: list[dict],
        conversation_history: list[dict] = None
    ) -> str:
        """Generate answer using Claude with RAG context"""

        # Build context
        context_text = "\n\n".join([
            f"[Source: {doc['source']}, Page: {doc.get('page', 'N/A')}]\n{doc['text']}"
            for doc in context_docs
        ])

        # Build messages
        messages = conversation_history or []
        messages.append({
            "role": "user",
            "content": f"""
Answer the following question using ONLY the provided context.
Always cite your sources with [Source: filename, Page: X].

<context>
{context_text}
</context>

<question>
{query}
</question>

Instructions:
- Answer in the same language as the question
- Be concise but complete
- Always cite sources for claims
- If context doesn't contain the answer, say so clearly
"""
        })

        # Call Claude
        response = await self.claude.messages.create(
            model="claude-sonnet-4.5-20250514",
            max_tokens=4000,
            messages=messages
        )

        return response.content[0].text

    async def query(
        self,
        user_query: str,
        conversation_history: list[dict] = None,
        filter: dict = None
    ) -> dict:
        """
        Full RAG pipeline: retrieve ‚Üí synthesize ‚Üí return with sources
        """

        # 1. Retrieve relevant documents
        retrieved_docs = await self.retrieve(
            query=user_query,
            top_k=5,
            filter=filter
        )

        # 2. Synthesize answer
        answer = await self.synthesize(
            query=user_query,
            context_docs=retrieved_docs,
            conversation_history=conversation_history
        )

        # 3. Return answer with sources
        return {
            "answer": answer,
            "sources": [
                {
                    "source": doc["source"],
                    "page": doc.get("page"),
                    "relevance_score": doc["score"]
                }
                for doc in retrieved_docs
            ]
        }
```

### 5.2 Chat API Endpoint

```python
# backend/app/api/chat.py

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from app.services.rag_engine import RAGEngine

router = APIRouter(prefix="/api/chat", tags=["chat"])
rag = RAGEngine()

class ChatRequest(BaseModel):
    message: str
    conversation_id: str | None = None
    filters: dict | None = None

class ChatResponse(BaseModel):
    answer: str
    sources: list[dict]
    conversation_id: str

@router.post("/", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    Chat endpoint with RAG
    """
    try:
        # Get conversation history if exists
        history = []
        if request.conversation_id:
            history = await get_conversation_history(request.conversation_id)

        # RAG query
        result = await rag.query(
            user_query=request.message,
            conversation_history=history,
            filter=request.filters
        )

        # Save to database
        conversation_id = request.conversation_id or create_new_conversation()
        await save_message(
            conversation_id=conversation_id,
            user_message=request.message,
            assistant_message=result["answer"],
            sources=result["sources"]
        )

        return ChatResponse(
            answer=result["answer"],
            sources=result["sources"],
            conversation_id=conversation_id
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

### 5.3 Frontend Chat Component

```typescript
// frontend/components/chat/ChatInterface.tsx

'use client';

import { useChat } from 'ai/react';
import { MessageList } from './MessageList';
import { ChatInput } from './ChatInput';

export function ChatInterface() {
  const { messages, input, handleInputChange, handleSubmit, isLoading } = useChat({
    api: '/api/chat',
  });

  return (
    <div className="flex flex-col h-screen max-w-4xl mx-auto">
      {/* Header */}
      <header className="p-4 border-b">
        <h1 className="text-2xl font-bold">Competitive Intelligence Assistant</h1>
        <p className="text-sm text-gray-600">Ask me anything about your competitors</p>
      </header>

      {/* Messages */}
      <div className="flex-1 overflow-y-auto p-4">
        <MessageList messages={messages} />
      </div>

      {/* Input */}
      <div className="p-4 border-t">
        <ChatInput
          value={input}
          onChange={handleInputChange}
          onSubmit={handleSubmit}
          isLoading={isLoading}
          placeholder="Ask about a competitor..."
        />
      </div>
    </div>
  );
}
```

---

## 6. D√©marrage Imm√©diat

### Option A: Je cr√©e tout le code maintenant

Je peux cr√©er TOUT le code de base (backend + frontend) dans ce repo, et vous pourrez:
1. Clone le repo
2. `docker-compose up` (d√©marre PostgreSQL)
3. `poetry install && poetry run uvicorn app.main:app` (backend)
4. `pnpm install && pnpm dev` (frontend)
5. Ouvrir http://localhost:3000
6. **Commencer √† utiliser l'app RAG** üöÄ

### Option B: Approche Progressive

On construit ensemble, √©tape par √©tape:
1. Je cr√©e la structure de base aujourd'hui
2. On teste ensemble demain
3. On it√®re selon vos besoins
4. On ajoute features progressivement

### Option C: Prototype Minimal d'Abord

Je cr√©e un **script Python standalone** (1 fichier, 200 lignes) qui fait RAG avec Claude + Pinecone, juste pour tester le concept aujourd'hui m√™me.

---

## 7. Quelle Option Pr√©f√©rez-Vous?

**A. "Cr√©e tout maintenant, je veux tester rapidement"**
‚Üí Je g√©n√®re la structure compl√®te + code de base + README

**B. "Commen√ßons petit, script standalone d'abord"**
‚Üí Je cr√©e un prototype minimal fonctionnel en 1 fichier

**C. "Construisons ensemble, √©tape par √©tape"**
‚Üí Je cr√©e la structure, puis on impl√©mente feature par feature

**D. "Explique-moi d'abord comment √ßa marche techniquement"**
‚Üí Je d√©taille chaque composant avant de coder

Quelle approche vous convient le mieux? üöÄ
