# Guide d'Ex√©cution - Analyse Prix Comp√©tition Dissan

**Date:** 18 novembre 2024
**Version:** 1.0 - PR√äT POUR EX√âCUTION

---

## ‚úÖ √âtat du Projet

### 100% du code impl√©ment√©!

- ‚úÖ **13 scrapers** impl√©ment√©s et op√©rationnels
- ‚úÖ **Infrastructure compl√®te** (rate limiting, checkpoints, logs, matchers)
- ‚úÖ **Excel exporter** avec 5 onglets d'analyse
- ‚úÖ **Scripts de consolidation** automatiques
- ‚úÖ **576 produits commerciaux** pr√™ts √† analyser
- ‚úÖ **13 comp√©titeurs** configur√©s

---

## üöÄ Guide d'Ex√©cution √âtape par √âtape

### √âTAPE 1: Pr√©paration (5 min)

```bash
cd /Users/jonathangaudette/market-intelligence/Dissan/price-scraper

# V√©rifier l'installation
npm run test:setup
```

**R√©sultat attendu:**
```
‚úÖ All tests passed! Setup is ready.
- 576 produits commerciaux charg√©s
- 13 comp√©titeurs configur√©s
- 13 scrapers op√©rationnels
```

---

### √âTAPE 2: Ajustement des S√©lecteurs CSS (CRITIQUE - 2-4h)

‚ö†Ô∏è **IMPORTANT:** Les s√©lecteurs CSS actuels sont des **estimations bas√©es sur des patterns e-commerce communs**. Ils devront √™tre affin√©s en testant sur les vrais sites.

#### Pour chaque site, proc√©dez comme suit:

**1. Swish.ca (Exemple d√©taill√©)**

```bash
# Ouvrir swish.ca dans le navigateur
open https://swish.ca

# Dans le navigateur:
# 1. Faire une recherche test (ex: "Rubbermaid")
# 2. F12 ‚Üí DevTools
# 3. Inspecter les √©l√©ments suivants:
```

**√âl√©ments √† identifier:**

| √âl√©ment | Description | Exemple de s√©lecteur |
|---------|-------------|----------------------|
| `searchBox` | Champ de recherche | `input[type="search"]`, `#search-input` |
| `searchButton` | Bouton rechercher | `button[type="submit"]`, `.search-btn` |
| `productList` | Container r√©sultats | `.product-grid .product-item`, `.products-list .product` |
| `productLink` | Lien vers produit | `a.product-link`, `.product-item a` |
| `productName` | Nom du produit | `.product-title`, `h3.product-name` |
| `productSku` | SKU affich√© | `.product-sku`, `.item-number` |
| `productPrice` | Prix | `.price`, `.product-price .amount` |
| `noResults` | Message "aucun r√©sultat" | `.no-results`, `.empty-state` |

**Astuce DevTools:**
```javascript
// Tester les s√©lecteurs dans la console:
document.querySelectorAll('.product-item')  // Doit retourner les produits
document.querySelector('.product-price')     // Doit retourner un √©l√©ment prix
```

**2. Mettre √† jour `competitors-config.json`**

```bash
# √âditer le fichier
code /Users/jonathangaudette/market-intelligence/Dissan/competitors-config.json

# Trouver la section "swish" et mettre √† jour les s√©lecteurs:
{
  "id": "swish",
  "selectors": {
    "searchBox": "#VRAI_SELECTEUR_ICI",
    "productList": ".VRAI_CONTAINER .product-item",
    "productName": ".VRAI_NOM_CLASSE",
    // ... etc
  }
}
```

**3. Tester le scraper**

```bash
cd /Users/jonathangaudette/market-intelligence/Dissan/price-scraper

# Test sur le site Swish uniquement
npm run scrape:site swish
```

**R√©sultat attendu:**
- Le scraper navigue sur swish.ca
- Recherche les produits
- Trouve au moins 60-70% des produits
- Pas d'erreurs massives dans les logs

**4. V√©rifier les r√©sultats**

```bash
# Voir les r√©sultats JSON
cat ../results/prix-par-site/swish-results.json | head -50

# Voir les logs
tail -50 data/logs/swish-2024-11-18.log
```

**R√©sultat attendu dans le JSON:**
```json
{
  "competitorId": "swish",
  "productsFound": 350,  // Au moins 60% de 576
  "productsNotFound": 220,
  "errors": 6,            // Moins de 5%
  "results": [
    {
      "sku": "ATL-12600",
      "found": true,
      "price": 15.99,
      "url": "https://swish.ca/products/...",
      "matchType": "sku"
    }
  ]
}
```

**5. R√©p√©ter pour les 12 autres sites**

Sites prioritaires √† faire en premier:
- ‚úÖ Swish (fait)
- üîÑ Grainger
- üîÑ ULINE
- üîÑ CleanItSupply

**Commandes de test:**
```bash
npm run scrape:site grainger
npm run scrape:site uline
npm run scrape:site cleanitsupply
```

---

### √âTAPE 3: Scraping Complet (30-40h automatique)

Une fois les s√©lecteurs ajust√©s et valid√©s pour au moins 3-5 sites:

#### Option A: Tout scraper en une fois (recommand√©)

```bash
cd /Users/jonathangaudette/market-intelligence/Dissan/price-scraper

# Lancer le scraping complet (30-40h)
# IMPORTANT: Peut tourner la nuit/weekend
nohup npm run scrape:all > ../logs/scraping-all.log 2>&1 &

# Suivre la progression en temps r√©el
tail -f ../logs/scraping-all.log
```

#### Option B: Par priorit√© (plus contr√¥l√©)

```bash
# Priorit√© 1 - Sites nationaux (5 sites √ó 2h = 10h)
npm run scrape:priority1

# V√©rifier les r√©sultats
ls -lh ../results/prix-par-site/

# Priorit√© 2 - E-commerce sp√©cialis√©s (5 sites √ó 3h = 15h)
npm run scrape:priority2

# Priorit√© 3 - Qu√©bec (3 sites √ó 3h = 9h)
npm run scrape:priority3
```

#### Option C: Site par site (d√©bug)

```bash
# Scraper un seul site √† la fois
npm run scrape:site swish      # ~2h
npm run scrape:site grainger   # ~3h
npm run scrape:site uline       # ~2h
# ... etc
```

**Monitoring pendant l'ex√©cution:**

```bash
# Terminal 1: Suivre les logs
tail -f data/logs/swish-2024-11-18.log

# Terminal 2: Surveiller les checkpoints
watch -n 60 'ls -lh data/checkpoints/'

# Terminal 3: Compter les r√©sultats
watch -n 300 'cat ../results/prix-par-site/swish-results.json | jq .productsFound'
```

**En cas d'interruption:**
Le scraper reprendra automatiquement au dernier checkpoint (tous les 50 produits). Relancer simplement:
```bash
npm run scrape:site swish  # Reprend o√π il s'√©tait arr√™t√©
```

---

### √âTAPE 4: Consolidation et G√©n√©ration Excel (5 min)

Une fois tous les scrapers ex√©cut√©s:

```bash
cd /Users/jonathangaudette/market-intelligence/Dissan/price-scraper

# G√©n√©rer le fichier Excel consolid√©
npm run analyze
```

**Ce qui se passe:**
1. Charge les 13 fichiers JSON de `results/prix-par-site/`
2. Merge les donn√©es par SKU
3. Calcule les statistiques (min, max, moyenne, √©cart %)
4. G√©n√®re 5 onglets Excel:
   - **Tous les produits** - Base de donn√©es compl√®te
   - **R√©sum√© par marque** - Stats par marque (ATL, RUB, etc.)
   - **R√©sum√© par comp√©titeur** - Taux de couverture par site
   - **Produits non trouv√©s** - Liste des produits < 3 sources
   - **Outliers de prix** - √âcarts de prix > 50%

**Fichier g√©n√©r√©:**
```
/Users/jonathangaudette/market-intelligence/Dissan/prix-competiteurs-final.xlsx
```

**R√©sultat console attendu:**
```
üìä Overall Statistics
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total products:          576
Products with prices:    485 (84.2%)
Products not found:      91
Average price:           $42.15
Price outliers (>50%):   23

üìä Statistics by Competitor
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
grainger             Found: 412 (71.5%)  Errors: 3
swish                Found: 387 (67.2%)  Errors: 5
uline                Found: 351 (60.9%)  Errors: 2
...
```

---

## üìä Analyse des R√©sultats

### Ouvrir le fichier Excel

```bash
open /Users/jonathangaudette/market-intelligence/Dissan/prix-competiteurs-final.xlsx
```

### Analyses cl√©s √† effectuer:

#### 1. Onglet "Tous les produits"
- ‚úÖ Filtrer par marque (ATL, RUB, SCA)
- ‚úÖ Trier par "√âcart %" d√©croissant ‚Üí identifier outliers
- ‚úÖ Filtrer "Nb Sources" ‚â• 5 ‚Üí produits avec bonne couverture

#### 2. Onglet "R√©sum√© par marque"
- ‚úÖ Identifier les marques avec meilleure/pire couverture
- ‚úÖ Comparer prix moyens par marque
- ‚úÖ Voir % de produits trouv√©s

#### 3. Onglet "R√©sum√© par comp√©titeur"
- ‚úÖ Identifier les sites les plus complets (>70% couverture)
- ‚úÖ Comparer prix moyens par site
- ‚úÖ Voir taux d'erreurs

#### 4. Onglet "Produits non trouv√©s"
- ‚úÖ Produits potentiellement exclusifs Dissan
- ‚úÖ Produits √† nomenclature diff√©rente
- ‚úÖ Opportunit√©s de diff√©renciation

#### 5. Onglet "Outliers de prix"
- ‚úÖ √âcarts de prix > 50% entre sites
- ‚úÖ Opportunit√©s d'optimisation tarifaire
- ‚úÖ Possibles erreurs de matching √† v√©rifier

---

## üîß Troubleshooting

### Probl√®me: Taux de produits trouv√©s < 50%

**Causes possibles:**
1. S√©lecteurs CSS incorrects
2. SKUs diff√©rents sur le site comp√©titeur
3. Produits non vendus par ce comp√©titeur

**Solutions:**
```bash
# 1. V√©rifier les logs d'erreurs
tail -100 data/logs/swish-2024-11-18-errors.log

# 2. Tester manuellement sur le site
# - Chercher un produit non trouv√©
# - V√©rifier si le SKU est identique
# - Ajuster les s√©lecteurs si n√©cessaire

# 3. Re-scraper le site
npm run scrape:site swish
```

### Probl√®me: Beaucoup d'erreurs 403 (Forbidden)

**Cause:** Rate limiting trop agressif

**Solution:**
```typescript
// √âditer Dissan/competitors-config.json
{
  "id": "swish",
  "rateLimiting": {
    "requestDelay": 4000,  // Augmenter de 2000 √† 4000ms
    "productDelay": 2000   // Augmenter de 1000 √† 2000ms
  }
}
```

### Probl√®me: Prix mal extraits (0, NaN, etc.)

**Cause:** S√©lecteur de prix incorrect ou format non reconnu

**Solution:**
```bash
# 1. Inspecter une page produit
open https://swish.ca/products/example

# 2. Identifier le vrai s√©lecteur de prix
# 3. Mettre √† jour competitors-config.json
{
  "selectors": {
    "productPrice": ".vrai-classe-prix"  // Mettre √† jour
  }
}

# 4. Re-scraper
npm run scrape:site swish
```

### Probl√®me: Scraper bloqu√©/gel√©

**Solution:**
```bash
# 1. V√©rifier les processus
ps aux | grep tsx

# 2. Killer si n√©cessaire
pkill -f "tsx src/main.ts"

# 3. V√©rifier le checkpoint
cat data/checkpoints/swish-checkpoint.json

# 4. Relancer (reprendra au checkpoint)
npm run scrape:site swish
```

---

## üìà M√©triques de Succ√®s

### Objectifs vis√©s:

| M√©trique | Objectif | Comment mesurer |
|----------|----------|-----------------|
| Taux de produits trouv√©s | >75% | Onglet "R√©sum√© par comp√©titeur" |
| Taux d'erreur | <5% | Logs + JSON results |
| Temps moyen/produit | <10s | Console output pendant scraping |
| Prix extraits correctement | >95% | Validation manuelle √©chantillon |
| Couverture comp√©titeurs | ‚â•8/13 sites | Nombre de sites >60% couverture |

### Validation Qualit√©:

**√âchantillon de 20 produits √† v√©rifier manuellement:**

```bash
# Extraire 20 SKUs au hasard
cat Dissan/produits-commerciaux.xlsx # Prendre 20 SKUs

# Pour chacun:
# 1. Ouvrir le site comp√©titeur
# 2. Chercher le produit
# 3. V√©rifier:
#    - Produit trouv√© = correct?
#    - Prix extrait = correct?
#    - URL fonctionne?
```

---

## üéØ Checklist Finale

### Avant de commencer:
- [ ] `npm run test:setup` passe sans erreur
- [ ] Playwright install√©: `npx playwright install chromium`
- [ ] S√©lecteurs CSS ajust√©s pour au moins 3 sites pilotes
- [ ] Tests manuels OK sur sites pilotes

### Pendant l'ex√©cution:
- [ ] Monitoring des logs actif
- [ ] Checkpoints sauvegard√©s r√©guli√®rement
- [ ] Taux d'erreur acceptable (<5%)
- [ ] Pas d'erreurs 403 massives

### Apr√®s l'ex√©cution:
- [ ] 13 fichiers JSON dans `results/prix-par-site/`
- [ ] Consolidation Excel r√©ussie
- [ ] Validation qualit√© sur √©chantillon
- [ ] M√©triques de succ√®s atteintes

---

## üí° Conseils Finaux

### Pour maximiser le taux de succ√®s:

1. **Commencer petit**: Tester d'abord sur 3-5 sites bien ajust√©s
2. **Valider rapidement**: V√©rifier les r√©sultats apr√®s chaque site
3. **Ajuster progressivement**: Am√©liorer les s√©lecteurs au fur et √† mesure
4. **Documenter les trouvailles**: Noter les patterns CSS qui fonctionnent
5. **√ätre patient**: 30-40h d'ex√©cution est normal pour 7,488 requ√™tes

### Pour le scraping nocturne:

```bash
# Lancer en arri√®re-plan
nohup npm run scrape:all > ../logs/scraping-$(date +%Y%m%d).log 2>&1 &

# Noter le PID
echo $! > /tmp/scraper.pid

# Le lendemain, v√©rifier
cat ../logs/scraping-20241118.log | tail -50
```

---

## üìû Support

**Documentation:**
- Plan complet: `PLAN-ANALYSE-PRIX-COMPETITION.md`
- Guide d√©taill√©: `GUIDE-NEXT-STEPS.md`
- Status: `STATUS.md`
- Ce guide: `GUIDE-EXECUTION.md`

**Logs:**
- Console: temps r√©el pendant ex√©cution
- Fichiers: `price-scraper/data/logs/`
- Erreurs: `*-errors.log`

**R√©sultats:**
- JSON par site: `results/prix-par-site/`
- Excel final: `prix-competiteurs-final.xlsx`

---

**Bonne chance avec l'ex√©cution! üöÄ**

Le projet est maintenant **100% pr√™t** pour l'analyse compl√®te des prix de la comp√©tition.
