# Architecture Multi-Tenant Pricing Intelligence avec Railway Worker - v2

**Date**: 2025-01-19
**Version**: 2.0 (R√©vis√©e - Production Ready)
**Auteur**: Architecture Technique - Module Pricing
**Status**: ‚úÖ Plan Valid√© - Corrections Critiques Int√©gr√©es
**R√©vision**: Corrections post-revue architecturale

---

## üìã Table des Mati√®res

1. [Vue d'Ensemble](#vue-densemble)
2. [Changelog v2](#changelog-v2)
3. [Contexte et Probl√©matique](#contexte-et-probl√©matique)
4. [Architecture Propos√©e](#architecture-propos√©e)
5. [Phase 0: Pr√©paration Database](#phase-0-pr√©paration-database)
6. [Composants D√©taill√©s](#composants-d√©taill√©s)
7. [S√©curit√© & Authentification](#s√©curit√©--authentification)
8. [Monitoring & Observabilit√©](#monitoring--observabilit√©)
9. [Gestion des Risques](#gestion-des-risques)
10. [Flux de Donn√©es](#flux-de-donn√©es)
11. [Impl√©mentation Technique](#impl√©mentation-technique)
12. [D√©ploiement Railway](#d√©ploiement-railway)
13. [Tests et Validation](#tests-et-validation)
14. [Co√ªts et Performance](#co√ªts-et-performance)
15. [Roadmap](#roadmap)
16. [Checklist Impl√©mentation](#checklist-impl√©mentation)

---

## üéØ Vue d'Ensemble

### Objectif

Cr√©er une **architecture multi-tenant production-ready** permettant:
- ‚úÖ Des **scrapers Playwright personnalis√©s par client** (Dissan, Akonovia, etc.)
- ‚úÖ **Pas de limitations Vercel** (timeout, taille, environnement)
- ‚úÖ **Interface unifi√©e** dans l'application Next.js
- ‚úÖ **Format de donn√©es standardis√©** pour tous les clients
- ‚úÖ **R√©utilisation du code Dissan existant** (13 scrapers Playwright)
- ‚úÖ **Monitoring et observabilit√©** int√©gr√©s (Sentry, logs structur√©s)
- ‚úÖ **S√©curit√© renforc√©e** (rate limiting, IP whitelist)
- ‚úÖ **Gestion d'erreurs robuste** (circuit breaker, retry, checkpointing)

### Solution

**Architecture hybride** avec worker externe:
```
Next.js (Vercel)  ‚Üí  Railway Worker (Playwright)  ‚Üí  PostgreSQL
     UI/API            Scraping Engine                Database
```

**Note Importante**: Cette v2 int√®gre des **corrections critiques** identifi√©es lors de la revue architecturale, notamment le batching logic (1 appel Railway = 1 concurrent, pas tous les concurrents).

---

## üìù Changelog v2

### Corrections Critiques

#### 1. **Architecture de Batching (CORRIG√â)**
- ‚ùå **v1**: 1 call Railway = TOUS les concurrents (impossible avec timeout)
- ‚úÖ **v2**: 1 call Railway = 1 concurrent, avec pagination si >100 produits

#### 2. **Database Schema (AJOUT√â)**
- üÜï Migration SQL pour ajouter `company_slug` √† `pricing_competitors`
- üÜï Phase 0 d√©di√©e √† la pr√©paration database

#### 3. **S√©curit√© (AJOUT√â)**
- üÜï Section compl√®te: Rate limiting, IP whitelist, JWT tokens
- üÜï Code samples pour express-rate-limit

#### 4. **Monitoring (AJOUT√â)**
- üÜï Structured logging avec Pino
- üÜï Error tracking avec Sentry
- üÜï Metrics endpoint pour Railway

#### 5. **Gestion des Risques (AJOUT√â)**
- üÜï Circuit breaker pattern
- üÜï Timeout management strategy
- üÜï Checkpointing pour recovery

#### 6. **Co√ªts R√©vis√©s (CORRIG√â)**
- ‚ùå **v1**: $0.06/mois (calcul incorrect)
- ‚úÖ **v2**: $0.69/mois (calcul r√©aliste: 6.24h/scan √ó 4 scans/mois)

#### 7. **Roadmap R√©vis√©e (AM√âLIOR√âE)**
- üÜï Phase 0: Migration database (0.5h)
- üÜï Phase 1.5: Batching logic (1h)
- üîÑ Temps estim√©s r√©alistes: MVP = 8h (au lieu de 6h)

---

## üîç Contexte et Probl√©matique

### Probl√®me Initial

**Vercel Serverless Functions ne supportent PAS Playwright** √† cause de:

1. **Limite de taille**: 50 MB (Playwright + Chromium = 200 MB)
2. **Timeout**: 10s (hobby), 300s max (pro) - pas assez pour scraper 500+ produits
3. **Environnement read-only**: Impossible d'installer binaires Chromium
4. **Cold starts**: Chaque invocation red√©marre tout

### Architecture Actuelle

```typescript
// src/lib/pricing/scraping-service.ts - ligne 274
private async executeScraping() {
  // TODO: Integrate with real scraper from /Dissan/price-scraper
  // For now, simulate scraping with mock data

  const mockProducts: ScrapedProduct[] = [
    { url: "...", name: "Mock Product 1", price: 9.99, ... }
  ];

  return { success: true, scrapedProducts: mockProducts };
}
```

**Probl√®me**: Mock data ne scrappe pas les vrais sites concurrents.

### Ce que nous avons d√©j√†

1. ‚úÖ **Code Dissan Playwright fonctionnel** (`/Dissan/price-scraper`)
   - 13 scrapers impl√©ment√©s (Swish, Grainger, Uline, etc.)
   - Architecture mature (base class, retry, checkpoints)
   - 576 produits √ó 13 concurrents test√©s

2. ‚úÖ **ScrapingService en place** (`src/lib/pricing/scraping-service.ts`)
   - Workflow complet (scan ‚Üí scrape ‚Üí match ‚Üí save)
   - MatchingService GPT-5 fonctionnel
   - UI dashboard pr√™te

3. ‚úÖ **Database schema** (`pricing_scans`, `pricing_matches`)

**Missing Link**: Connecter le scraper Playwright au ScrapingService.

---

## üèóÔ∏è Architecture Propos√©e

### Diagramme Global

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    NEXT.JS APP (VERCEL)                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  UI Dashboard (React)                                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - /companies/[slug]/pricing                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Products List                                           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Product Detail Page                                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - "Lancer scan" button                                    ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                ‚Üì                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  API Routes                                                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  POST /api/companies/[slug]/pricing/scans                  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                ‚Üì                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  ScrapingService                                           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - scrapeAllCompetitors(companyId)                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - scrapeCompetitor(competitorId) ‚Üê LOOP PER COMPETITOR   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - executeScraping() ‚Üê MODIFIED HERE                      ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                ‚Üì                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  WorkerClient (NEW)                                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - scrape(competitorId, products[])                       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - POST https://worker.railway.app/api/scrape             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Pagination if products.length > 100                     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
                               ‚îÇ HTTPS Request (PER COMPETITOR)
                               ‚îÇ { competitorId, products[0..100] }
                               ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ               RAILWAY WORKER (Node.js + Playwright)              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Express Server + Middleware                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Rate Limiting (100 req/15min per IP)                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - API Key validation                                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Structured Logging (Pino)                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Error Tracking (Sentry)                                 ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                ‚Üì                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  POST /api/scrape                                          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Validates request (Zod)                                 ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Logs to Sentry transaction                              ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                ‚Üì                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  ScraperFactory                                            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - getScraperForCompany(companySlug)                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Returns: DissanScraper | AkonoviaScraper | ...         ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                ‚Üì                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  DissanScraper extends BasePlaywrightScraper               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Code from /Dissan/price-scraper (reused)               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - scrapeCompetitor(competitor, products[])                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Returns: ScrapedProduct[]                               ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                ‚Üì                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Playwright Browser                                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Chromium (headless)                                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Scrapes competitor website                              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - SKU matching, name matching                             ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
                               ‚îÇ HTTP Response
                               ‚îÇ { success, scrapedProducts[] }
                               ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    NEXT.JS APP (VERCEL)                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  MatchingService (GPT-5)                                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - matchProducts(scrapedProducts)                          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - AI matching with confidence scores                      ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                ‚Üì                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  PostgreSQL (Vercel Postgres)                              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - pricing_scans                                           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - pricing_matches                                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - pricing_products                                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - pricing_competitors (with company_slug)                 ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Principes Cl√©s

1. **S√©paration des responsabilit√©s**
   - Next.js: UI, API, business logic, AI matching
   - Railway: Heavy compute (Playwright scraping)

2. **Scraping s√©quentiel par concurrent** (CORRECTION v2)
   - ‚úÖ 1 appel Railway = 1 concurrent
   - ‚úÖ Loop S√âQUENTIEL dans `scrapeAllCompetitors()`
   - ‚úÖ Pagination automatique si >100 produits/concurrent

3. **Interface contractuelle**
   - HTTP API avec types stricts (Zod validation)
   - Format de donn√©es standardis√© `ScrapedProduct[]`

4. **Flexibilit√© multi-tenant**
   - Chaque client peut avoir son scraper (Playwright, Apify, API)
   - ScraperFactory s√©lectionne dynamiquement

5. **Observabilit√© int√©gr√©e** (NOUVEAU v2)
   - Structured logging (Pino)
   - Error tracking (Sentry)
   - Metrics endpoint (/metrics)

6. **S√©curit√© renforc√©e** (NOUVEAU v2)
   - Rate limiting
   - IP whitelist
   - API Key validation

---

## üóÑÔ∏è Phase 0: Pr√©paration Database

### ‚ö†Ô∏è CRITIQUE: Migration `company_slug`

**Probl√®me**: Le schema `pricing_competitors` n'a pas de champ `company_slug`, mais le Railway worker en a besoin pour s√©lectionner le bon scraper via `ScraperFactory`.

**Solution**: Migration SQL AVANT toute impl√©mentation.

#### Migration SQL

```sql
-- migrations/0015_add_company_slug_to_competitors.sql

-- 1. Add column (nullable initially)
ALTER TABLE pricing_competitors
ADD COLUMN company_slug VARCHAR(255);

-- 2. Populate from companies table
UPDATE pricing_competitors pc
SET company_slug = c.slug
FROM companies c
WHERE pc.company_id = c.id;

-- 3. Verify all rows have value
SELECT COUNT(*) FROM pricing_competitors WHERE company_slug IS NULL;
-- Should return 0

-- 4. Make it NOT NULL
ALTER TABLE pricing_competitors
ALTER COLUMN company_slug SET NOT NULL;

-- 5. Add index for performance
CREATE INDEX idx_pricing_competitors_company_slug
ON pricing_competitors(company_slug);

-- 6. Verify
SELECT
  pc.id,
  pc.name AS competitor_name,
  pc.company_slug,
  c.slug AS company_slug_from_join
FROM pricing_competitors pc
INNER JOIN companies c ON pc.company_id = c.id
LIMIT 5;
```

#### Script Node.js pour Migration

```javascript
// scripts/run-migration-0015.mjs
import { db } from './src/db/index.js';
import { sql } from 'drizzle-orm';

async function runMigration() {
  console.log('üîÑ Running migration: Add company_slug to pricing_competitors');

  try {
    // 1. Add column
    await db.execute(sql`
      ALTER TABLE pricing_competitors
      ADD COLUMN IF NOT EXISTS company_slug VARCHAR(255)
    `);
    console.log('‚úÖ Column added');

    // 2. Populate
    await db.execute(sql`
      UPDATE pricing_competitors pc
      SET company_slug = c.slug
      FROM companies c
      WHERE pc.company_id = c.id
      AND pc.company_slug IS NULL
    `);
    console.log('‚úÖ Data populated');

    // 3. Verify
    const nullCount = await db.execute(sql`
      SELECT COUNT(*) as count
      FROM pricing_competitors
      WHERE company_slug IS NULL
    `);

    if (nullCount.rows[0].count > 0) {
      throw new Error(`Found ${nullCount.rows[0].count} rows with NULL company_slug`);
    }
    console.log('‚úÖ No NULL values');

    // 4. Make NOT NULL
    await db.execute(sql`
      ALTER TABLE pricing_competitors
      ALTER COLUMN company_slug SET NOT NULL
    `);
    console.log('‚úÖ Column set to NOT NULL');

    // 5. Add index
    await db.execute(sql`
      CREATE INDEX IF NOT EXISTS idx_pricing_competitors_company_slug
      ON pricing_competitors(company_slug)
    `);
    console.log('‚úÖ Index created');

    console.log('üéâ Migration completed successfully!');
  } catch (error) {
    console.error('‚ùå Migration failed:', error);
    process.exit(1);
  }
}

runMigration();
```

#### Ex√©cution

```bash
# Dev
npm run db:generate
npm run db:migrate

# Production (Vercel)
node scripts/run-migration-0015.mjs
```

**‚è±Ô∏è Dur√©e estim√©e**: 30 minutes (inclus tests)

---

## üîß Composants D√©taill√©s

### 1. WorkerClient (Nouveau - Next.js)

**Fichier**: `src/lib/pricing/worker-client.ts`

```typescript
import { z } from 'zod';

// ============================================================================
// Types & Validation
// ============================================================================

const ScrapedProductSchema = z.object({
  url: z.string().url(),
  name: z.string(),
  sku: z.string().optional(),
  price: z.number().positive(),
  currency: z.string().default('CAD'),
  inStock: z.boolean().default(true),
  imageUrl: z.string().url().optional(),
  characteristics: z.record(z.any()).optional(),
});

const ScrapeRequestSchema = z.object({
  companyId: z.string(),
  companySlug: z.string(),
  competitorId: z.string(),
  competitorName: z.string(),
  competitorUrl: z.string().url(),
  products: z.array(z.object({
    id: z.string(),
    sku: z.string(),
    name: z.string(),
    brand: z.string().nullable(),
    category: z.string().nullable(),
  })),
  // NEW v2: Batch info for pagination
  batchInfo: z.object({
    batchNumber: z.number(),
    totalBatches: z.number(),
  }).optional(),
});

const ScrapeResponseSchema = z.object({
  success: z.boolean(),
  scrapedProducts: z.array(ScrapedProductSchema),
  productsScraped: z.number(),
  productsFailed: z.number(),
  errors: z.array(z.object({
    url: z.string(),
    error: z.string(),
    timestamp: z.string(),
  })),
  metadata: z.object({
    duration: z.number(), // milliseconds
    scraperType: z.enum(['playwright', 'apify', 'api']),
    workerStatus: z.enum(['UP', 'DOWN']).optional(), // NEW v2
  }),
});

export type ScrapeRequest = z.infer<typeof ScrapeRequestSchema>;
export type ScrapeResponse = z.infer<typeof ScrapeResponseSchema>;
export type ScrapedProduct = z.infer<typeof ScrapedProductSchema>;

// ============================================================================
// Worker Client
// ============================================================================

export class WorkerClient {
  private baseUrl: string;
  private apiKey: string;
  private timeout: number;
  private maxRetries: number; // NEW v2

  constructor() {
    this.baseUrl = process.env.RAILWAY_WORKER_URL || 'http://localhost:3001';
    this.apiKey = process.env.RAILWAY_WORKER_API_KEY || '';
    this.timeout = 600000; // 10 minutes per competitor
    this.maxRetries = 2; // NEW v2: Retry failed requests
  }

  /**
   * Trigger scraping job on Railway worker
   * NEW v2: Automatic pagination if >100 products
   */
  async scrape(request: ScrapeRequest): Promise<ScrapeResponse> {
    const BATCH_SIZE = 100;

    // Pagination logic (NEW v2)
    if (request.products.length > BATCH_SIZE) {
      console.log(`[WorkerClient] Paginating ${request.products.length} products into batches of ${BATCH_SIZE}`);

      const allResults: ScrapedProduct[] = [];
      const allErrors: any[] = [];
      let totalScraped = 0;
      let totalFailed = 0;

      const totalBatches = Math.ceil(request.products.length / BATCH_SIZE);

      for (let i = 0; i < request.products.length; i += BATCH_SIZE) {
        const batch = request.products.slice(i, i + BATCH_SIZE);
        const batchNumber = i / BATCH_SIZE;

        console.log(`[WorkerClient] Processing batch ${batchNumber + 1}/${totalBatches}`);

        const batchRequest = {
          ...request,
          products: batch,
          batchInfo: {
            batchNumber,
            totalBatches,
          },
        };

        const batchResult = await this.scrapeInternal(batchRequest);

        allResults.push(...batchResult.scrapedProducts);
        allErrors.push(...batchResult.errors);
        totalScraped += batchResult.productsScraped;
        totalFailed += batchResult.productsFailed;
      }

      return {
        success: true,
        scrapedProducts: allResults,
        productsScraped: totalScraped,
        productsFailed: totalFailed,
        errors: allErrors,
        metadata: {
          duration: 0, // Aggregated duration not tracked
          scraperType: 'playwright',
        },
      };
    } else {
      return this.scrapeInternal(request);
    }
  }

  /**
   * Internal scrape method (single batch)
   * NEW v2: Retry logic + circuit breaker
   */
  private async scrapeInternal(request: ScrapeRequest, retryCount = 0): Promise<ScrapeResponse> {
    console.log(`[WorkerClient] Calling Railway worker for ${request.competitorName}`);
    console.log(`[WorkerClient] Products to scrape: ${request.products.length}`);

    // Validate request
    const validatedRequest = ScrapeRequestSchema.parse(request);

    try {
      const controller = new AbortController();
      const timeoutId = setTimeout(() => controller.abort(), this.timeout);

      const response = await fetch(`${this.baseUrl}/api/scrape`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'X-API-Key': this.apiKey,
        },
        body: JSON.stringify(validatedRequest),
        signal: controller.signal,
      });

      clearTimeout(timeoutId);

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(
          `Railway worker error (${response.status}): ${errorText}`
        );
      }

      const data = await response.json();

      // Validate response
      const validatedResponse = ScrapeResponseSchema.parse(data);

      console.log(`[WorkerClient] Success! Scraped ${validatedResponse.productsScraped} products`);
      console.log(`[WorkerClient] Duration: ${validatedResponse.metadata.duration}ms`);

      return validatedResponse;
    } catch (error: any) {
      if (error.name === 'AbortError') {
        console.error(`[WorkerClient] Timeout after ${this.timeout}ms`);
      }

      // NEW v2: Retry logic
      if (retryCount < this.maxRetries) {
        console.log(`[WorkerClient] Retrying (${retryCount + 1}/${this.maxRetries})...`);
        await new Promise(resolve => setTimeout(resolve, 2000 * (retryCount + 1))); // Exponential backoff
        return this.scrapeInternal(request, retryCount + 1);
      }

      console.error('[WorkerClient] Error calling Railway worker:', error);

      // NEW v2: Return graceful error response instead of throwing
      return {
        success: false,
        scrapedProducts: [],
        productsScraped: 0,
        productsFailed: request.products.length,
        errors: [{
          url: 'WORKER_ERROR',
          error: error.message,
          timestamp: new Date().toISOString(),
        }],
        metadata: {
          duration: 0,
          scraperType: 'playwright',
          workerStatus: 'DOWN',
        },
      };
    }
  }

  /**
   * Health check
   */
  async healthCheck(): Promise<boolean> {
    try {
      const response = await fetch(`${this.baseUrl}/health`, {
        method: 'GET',
        headers: { 'X-API-Key': this.apiKey },
        signal: AbortSignal.timeout(5000), // 5s timeout
      });
      return response.ok;
    } catch (error) {
      console.error('[WorkerClient] Health check failed:', error);
      return false;
    }
  }
}
```

---

### 2. ScrapingService Modifi√© (CORRECTION v2)

**Fichier**: `src/lib/pricing/scraping-service.ts`

**Changements critiques**:
- ‚úÖ Loop S√âQUENTIEL dans `scrapeAllCompetitors()` (1 concurrent √† la fois)
- ‚úÖ Appel WorkerClient par concurrent (pas tous ensemble)
- ‚úÖ Gestion d'erreurs am√©lior√©e

```typescript
import { WorkerClient, ScrapedProduct } from './worker-client';
import { MatchingService } from './matching-service';
import { db } from '@/db';
import {
  pricingScans,
  pricingCompetitors,
  pricingProducts
} from '@/db/schema';
import { eq, and } from 'drizzle-orm';
import { createId } from '@paralleldrive/cuid2';

export class ScrapingService {
  private matchingService: MatchingService;
  private workerClient: WorkerClient;

  constructor() {
    this.matchingService = new MatchingService();
    this.workerClient = new WorkerClient();
  }

  /**
   * Scrape ALL active competitors for a company
   * NEW v2: SEQUENTIAL scraping (1 competitor at a time)
   */
  async scrapeAllCompetitors(companyId: string): Promise<{
    success: boolean;
    totalCompetitors: number;
    successfulScans: number;
    failedScans: number;
  }> {
    console.log(`[ScrapingService] Starting scrape for all competitors (company: ${companyId})`);

    // Fetch active competitors
    const competitors = await db
      .select()
      .from(pricingCompetitors)
      .where(
        and(
          eq(pricingCompetitors.companyId, companyId),
          eq(pricingCompetitors.isActive, true)
        )
      );

    console.log(`[ScrapingService] Found ${competitors.length} active competitors`);

    if (competitors.length === 0) {
      return {
        success: true,
        totalCompetitors: 0,
        successfulScans: 0,
        failedScans: 0,
      };
    }

    let successfulScans = 0;
    let failedScans = 0;

    // CRITICAL v2: SEQUENTIAL loop (not parallel!)
    // 1 call Railway = 1 competitor
    for (const competitor of competitors) {
      try {
        console.log(`[ScrapingService] Scraping competitor: ${competitor.name}`);

        const result = await this.scrapeCompetitor(competitor.id);

        if (result.success) {
          successfulScans++;
          console.log(`[ScrapingService] ‚úÖ Success: ${competitor.name} (${result.productsScraped} products)`);
        } else {
          failedScans++;
          console.log(`[ScrapingService] ‚ùå Failed: ${competitor.name}`);
        }
      } catch (error: any) {
        failedScans++;
        console.error(`[ScrapingService] ‚ùå Error scraping ${competitor.name}:`, error);
      }
    }

    console.log(`[ScrapingService] Completed all competitors`);
    console.log(`[ScrapingService] Successful: ${successfulScans}, Failed: ${failedScans}`);

    return {
      success: true,
      totalCompetitors: competitors.length,
      successfulScans,
      failedScans,
    };
  }

  /**
   * Scrape a single competitor
   */
  async scrapeCompetitor(competitorId: string): Promise<{
    success: boolean;
    scanId: string;
    productsScraped: number;
    productsFailed: number;
    errors: any[];
  }> {
    // Fetch competitor with company info
    const [competitor] = await db
      .select({
        id: pricingCompetitors.id,
        name: pricingCompetitors.name,
        websiteUrl: pricingCompetitors.websiteUrl,
        companyId: pricingCompetitors.companyId,
        companySlug: pricingCompetitors.companySlug, // NEW: from Phase 0 migration
      })
      .from(pricingCompetitors)
      .where(eq(pricingCompetitors.id, competitorId))
      .limit(1);

    if (!competitor) {
      throw new Error(`Competitor not found: ${competitorId}`);
    }

    // Create scan record
    const scanId = createId();
    const logs: LogEvent[] = [];

    await db.insert(pricingScans).values({
      id: scanId,
      companyId: competitor.companyId,
      status: 'in_progress',
      currentStep: 'Initializing scan',
      progressCurrent: 0,
      progressTotal: 100,
      logs: logs,
    });

    try {
      // Execute scraping via Railway worker
      const result = await this.executeScraping(competitor, scanId, logs);

      // If scraping succeeded, run AI matching
      if (result.success && result.scrapedProducts.length > 0) {
        logs.push({
          timestamp: new Date().toISOString(),
          type: 'info',
          message: `Running GPT-5 matching for ${result.scrapedProducts.length} products`,
        });

        await db
          .update(pricingScans)
          .set({
            currentStep: 'AI Matching with GPT-5',
            progressCurrent: 85,
            logs: logs,
            updatedAt: new Date(),
          })
          .where(eq(pricingScans.id, scanId));

        const matches = await this.matchingService.matchProducts(
          competitor.companyId,
          competitor.id,
          result.scrapedProducts
        );

        logs.push({
          timestamp: new Date().toISOString(),
          type: 'success',
          message: `AI Matching completed: ${matches.length} matches found`,
        });

        await db
          .update(pricingScans)
          .set({
            status: 'completed',
            currentStep: 'Completed',
            progressCurrent: 100,
            productsScraped: result.productsScraped,
            productsMatched: matches.length,
            errors: result.errors,
            logs: logs,
            completedAt: new Date(),
            updatedAt: new Date(),
          })
          .where(eq(pricingScans.id, scanId));

        return {
          success: true,
          scanId,
          productsScraped: result.productsScraped,
          productsFailed: result.productsFailed,
          errors: result.errors,
        };
      } else {
        // Scraping failed
        await db
          .update(pricingScans)
          .set({
            status: 'failed',
            currentStep: 'Failed',
            errors: result.errors,
            logs: logs,
            completedAt: new Date(),
            updatedAt: new Date(),
          })
          .where(eq(pricingScans.id, scanId));

        return {
          success: false,
          scanId,
          productsScraped: result.productsScraped,
          productsFailed: result.productsFailed,
          errors: result.errors,
        };
      }
    } catch (error: any) {
      logs.push({
        timestamp: new Date().toISOString(),
        type: 'error',
        message: `Fatal error: ${error.message}`,
      });

      await db
        .update(pricingScans)
        .set({
          status: 'failed',
          currentStep: 'Error',
          logs: logs,
          errors: [{ url: 'SYSTEM', error: error.message, timestamp: new Date() }],
          completedAt: new Date(),
          updatedAt: new Date(),
        })
        .where(eq(pricingScans.id, scanId));

      throw error;
    }
  }

  /**
   * Execute the actual scraping logic
   * MODIFIED v2: Calls Railway worker instead of mock data
   */
  private async executeScraping(
    competitor: any,
    scanId: string,
    logs: LogEvent[]
  ): Promise<{
    success: boolean;
    scrapedProducts: ScrapedProduct[];
    productsScraped: number;
    productsFailed: number;
    errors: ScrapingError[];
  }> {
    try {
      logs.push({
        timestamp: new Date().toISOString(),
        type: 'info',
        message: `Calling Railway worker for ${competitor.name}`,
      });

      await db
        .update(pricingScans)
        .set({
          currentStep: 'Fetching products from Railway worker',
          progressCurrent: 10,
          logs: logs,
          updatedAt: new Date(),
        })
        .where(eq(pricingScans.id, scanId));

      // Fetch active products for this company
      const activeProducts = await db
        .select({
          id: pricingProducts.id,
          sku: pricingProducts.sku,
          name: pricingProducts.name,
          brand: pricingProducts.brand,
          category: pricingProducts.category,
        })
        .from(pricingProducts)
        .where(
          and(
            eq(pricingProducts.companyId, competitor.companyId),
            eq(pricingProducts.isActive, true)
          )
        );

      console.log(
        `[ScrapingService] Sending ${activeProducts.length} products to Railway worker`
      );

      // Call Railway worker
      const result = await this.workerClient.scrape({
        companyId: competitor.companyId,
        companySlug: competitor.companySlug,
        competitorId: competitor.id,
        competitorName: competitor.name,
        competitorUrl: competitor.websiteUrl,
        products: activeProducts,
      });

      logs.push({
        timestamp: new Date().toISOString(),
        type: result.success ? 'success' : 'warning',
        message: result.success
          ? `Railway worker completed: ${result.productsScraped} products scraped`
          : `Railway worker completed with errors`,
        metadata: {
          productsFound: result.productsScraped,
          duration: result.metadata.duration,
          scraperType: result.metadata.scraperType,
        },
      });

      await db
        .update(pricingScans)
        .set({
          currentStep: 'Processing scraped data',
          progressCurrent: 80,
          productsScraped: result.productsScraped,
          logs: logs,
          updatedAt: new Date(),
        })
        .where(eq(pricingScans.id, scanId));

      return {
        success: result.success,
        scrapedProducts: result.scrapedProducts,
        productsScraped: result.productsScraped,
        productsFailed: result.productsFailed,
        errors: result.errors.map(e => ({
          url: e.url,
          error: e.error,
          timestamp: new Date(e.timestamp),
        })),
      };
    } catch (error: any) {
      const scrapingError: ScrapingError = {
        url: competitor.websiteUrl,
        error: error.message,
        timestamp: new Date(),
      };

      logs.push({
        timestamp: new Date().toISOString(),
        type: 'error',
        message: `Railway worker error: ${error.message}`,
      });

      return {
        success: false,
        scrapedProducts: [],
        productsScraped: 0,
        productsFailed: 1,
        errors: [scrapingError],
      };
    }
  }
}

interface LogEvent {
  timestamp: string;
  type: 'info' | 'success' | 'warning' | 'error';
  message: string;
  metadata?: Record<string, any>;
}

interface ScrapingError {
  url: string;
  error: string;
  timestamp: Date;
}
```

---

### 3. Railway Worker - Express Server (NOUVEAU v2: Monitoring)

**Fichier**: `worker/src/index.ts`

```typescript
import express from 'express';
import { z } from 'zod';
import rateLimit from 'express-rate-limit'; // NEW v2
import pino from 'pino'; // NEW v2
import * as Sentry from '@sentry/node'; // NEW v2
import { ScraperFactory } from './scrapers/factory';
import { ScrapeRequestSchema, ScrapeResponseSchema } from './types';

const app = express();
const PORT = process.env.PORT || 3001;

// ============================================================================
// NEW v2: Structured Logging
// ============================================================================

const logger = pino({
  level: process.env.LOG_LEVEL || 'info',
  transport: process.env.NODE_ENV === 'development'
    ? { target: 'pino-pretty' }
    : undefined,
});

// ============================================================================
// NEW v2: Sentry Error Tracking
// ============================================================================

Sentry.init({
  dsn: process.env.SENTRY_DSN,
  environment: process.env.NODE_ENV || 'development',
  tracesSampleRate: 1.0,
});

// ============================================================================
// Middleware
// ============================================================================

app.use(express.json({ limit: '10mb' }));

// NEW v2: Rate Limiting
const limiter = rateLimit({
  windowMs: 15 * 60 * 1000, // 15 minutes
  max: 100, // max 100 requests per IP
  message: 'Too many requests from this IP, please try again later',
  standardHeaders: true,
  legacyHeaders: false,
});

app.use('/api/scrape', limiter);

// Auth middleware
app.use((req, res, next) => {
  const apiKey = req.headers['x-api-key'];
  if (!apiKey || apiKey !== process.env.API_KEY) {
    logger.warn({ ip: req.ip, path: req.path }, 'Unauthorized request');
    return res.status(401).json({ error: 'Unauthorized' });
  }
  next();
});

// NEW v2: Request logging
app.use((req, res, next) => {
  logger.info({
    method: req.method,
    path: req.path,
    ip: req.ip,
  }, 'Incoming request');
  next();
});

// ============================================================================
// Routes
// ============================================================================

// Health check
app.get('/health', (req, res) => {
  res.json({
    status: 'healthy',
    timestamp: new Date().toISOString(),
    uptime: process.uptime(),
  });
});

// NEW v2: Metrics endpoint (Prometheus format)
app.get('/metrics', (req, res) => {
  // TODO: Implement prometheus metrics
  res.send('# Metrics placeholder');
});

// Main scraping endpoint
app.post('/api/scrape', async (req, res) => {
  const startTime = Date.now();

  // NEW v2: Sentry transaction
  const transaction = Sentry.startTransaction({
    op: 'scrape',
    name: 'POST /api/scrape',
  });

  try {
    logger.info('Received scrape request');

    // Validate request
    const request = ScrapeRequestSchema.parse(req.body);

    logger.info({
      companySlug: request.companySlug,
      competitorName: request.competitorName,
      productsCount: request.products.length,
      batchInfo: request.batchInfo,
    }, 'Scrape request validated');

    // Get appropriate scraper
    const scraper = ScraperFactory.getScraperForCompany(request.companySlug);

    logger.info({
      scraperType: scraper.constructor.name,
    }, 'Scraper selected');

    // Execute scraping
    const result = await scraper.scrapeCompetitor({
      competitorId: request.competitorId,
      competitorName: request.competitorName,
      competitorUrl: request.competitorUrl,
      products: request.products,
    });

    const duration = Date.now() - startTime;

    // Build response
    const response = {
      success: true,
      scrapedProducts: result.scrapedProducts,
      productsScraped: result.productsScraped,
      productsFailed: result.productsFailed,
      errors: result.errors,
      metadata: {
        duration,
        scraperType: scraper.scraperType,
      },
    };

    logger.info({
      duration,
      productsScraped: result.productsScraped,
      productsFailed: result.productsFailed,
    }, 'Scraping completed successfully');

    transaction.setStatus('ok');
    transaction.finish();

    res.json(response);
  } catch (error: any) {
    const duration = Date.now() - startTime;

    logger.error({
      error: error.message,
      stack: error.stack,
      duration,
    }, 'Scraping failed');

    // NEW v2: Report to Sentry
    Sentry.captureException(error, {
      tags: {
        endpoint: '/api/scrape',
      },
    });

    transaction.setStatus('internal_error');
    transaction.finish();

    if (error instanceof z.ZodError) {
      return res.status(400).json({
        error: 'Invalid request',
        details: error.errors,
      });
    }

    res.status(500).json({
      error: 'Internal server error',
      message: error.message,
    });
  }
});

// ============================================================================
// Error Handler
// ============================================================================

// NEW v2: Global error handler
app.use((err: any, req: any, res: any, next: any) => {
  logger.error({
    error: err.message,
    stack: err.stack,
    path: req.path,
  }, 'Unhandled error');

  Sentry.captureException(err);

  res.status(500).json({
    error: 'Internal server error',
    message: err.message,
  });
});

// ============================================================================
// Start Server
// ============================================================================

app.listen(PORT, () => {
  logger.info({
    port: PORT,
    environment: process.env.NODE_ENV || 'development',
    nodeVersion: process.version,
  }, 'Server started');
});
```

---

## üîê S√©curit√© & Authentification

### 1. Rate Limiting

**Objectif**: Pr√©venir les abus et attaques DDoS.

**Impl√©mentation**:

```typescript
// worker/src/middleware/rate-limiter.ts
import rateLimit from 'express-rate-limit';
import RedisStore from 'rate-limit-redis'; // Optional: For distributed rate limiting
import { createClient } from 'redis';

// Option 1: In-memory (simple, for single Railway instance)
export const scraperRateLimiter = rateLimit({
  windowMs: 15 * 60 * 1000, // 15 minutes
  max: 100, // Max 100 requests per IP
  message: 'Too many scraping requests, please try again later',
  standardHeaders: true,
  legacyHeaders: false,
  // NEW: Custom key generator (per API key instead of IP)
  keyGenerator: (req) => {
    return req.headers['x-api-key'] as string || req.ip;
  },
});

// Option 2: Redis-backed (scalable, for multiple Railway instances)
const redisClient = createClient({
  url: process.env.REDIS_URL,
});

export const scraperRateLimiterRedis = rateLimit({
  windowMs: 15 * 60 * 1000,
  max: 100,
  store: new RedisStore({
    // @ts-ignore
    client: redisClient,
    prefix: 'rl:scraper:',
  }),
});

// Usage in index.ts
app.use('/api/scrape', scraperRateLimiter);
```

### 2. IP Whitelist (Vercel IPs Only)

**Objectif**: N'autoriser que les requ√™tes provenant de Vercel.

**Impl√©mentation**:

```typescript
// worker/src/middleware/ip-whitelist.ts
import { Request, Response, NextFunction } from 'express';

// Vercel IP ranges (updated 2024)
// Source: https://vercel.com/docs/concepts/edge-network/headers#x-forwarded-for
const VERCEL_IP_RANGES = [
  '76.76.21.0/24',
  '76.76.21.21', // Example specific IP
  // Add all Vercel IPs from their docs
];

// Helper: Check if IP is in CIDR range
function ipInRange(ip: string, cidr: string): boolean {
  // Simplified implementation
  // Use library like 'ip-range-check' for production
  return cidr.includes(ip);
}

export function ipWhitelistMiddleware(req: Request, res: Response, next: NextFunction) {
  const clientIP = req.headers['x-forwarded-for'] || req.ip || req.connection.remoteAddress;

  console.log(`[IP Whitelist] Checking IP: ${clientIP}`);

  // Allow localhost in development
  if (process.env.NODE_ENV === 'development') {
    return next();
  }

  // Check against whitelist
  const isAllowed = VERCEL_IP_RANGES.some(range => {
    if (range.includes('/')) {
      return ipInRange(clientIP as string, range);
    }
    return clientIP === range;
  });

  if (!isAllowed) {
    console.warn(`[IP Whitelist] Rejected IP: ${clientIP}`);
    return res.status(403).json({ error: 'Forbidden: IP not whitelisted' });
  }

  next();
}

// Usage in index.ts
app.use('/api/scrape', ipWhitelistMiddleware);
```

### 3. JWT Tokens (Phase 2 - Roadmap)

**Avantages sur API Key statique**:
- ‚úÖ Expiration automatique (1 heure)
- ‚úÖ Claims contextuels (companyId, userId)
- ‚úÖ Rotation facile des secrets

**Impl√©mentation**:

```typescript
// Next.js: Generate JWT
import jwt from 'jsonwebtoken';

export async function generateWorkerToken(companyId: string): Promise<string> {
  const payload = {
    companyId,
    iat: Math.floor(Date.now() / 1000),
  };

  const token = jwt.sign(payload, process.env.JWT_SECRET!, {
    expiresIn: '1h', // 1 hour expiration
    issuer: 'market-intelligence-vercel',
  });

  return token;
}

// Railway Worker: Verify JWT
import jwt from 'jsonwebtoken';

export function jwtAuthMiddleware(req: Request, res: Response, next: NextFunction) {
  const authHeader = req.headers['authorization'];

  if (!authHeader || !authHeader.startsWith('Bearer ')) {
    return res.status(401).json({ error: 'Missing or invalid authorization header' });
  }

  const token = authHeader.substring(7); // Remove "Bearer "

  try {
    const payload = jwt.verify(token, process.env.JWT_SECRET!) as {
      companyId: string;
      iat: number;
    };

    // Attach payload to request
    (req as any).auth = payload;

    next();
  } catch (error) {
    if (error instanceof jwt.TokenExpiredError) {
      return res.status(401).json({ error: 'Token expired' });
    }

    return res.status(401).json({ error: 'Invalid token' });
  }
}

// Usage
app.use('/api/scrape', jwtAuthMiddleware);
```

**Roadmap**: Impl√©menter JWT en Phase 2 apr√®s validation du MVP avec API Key.

---

## üìä Monitoring & Observabilit√©

### 1. Structured Logging avec Pino

**Objectif**: Logs structur√©s et facilement searchables.

**Configuration compl√®te**:

```typescript
// worker/src/utils/logger.ts
import pino from 'pino';

const logger = pino({
  level: process.env.LOG_LEVEL || 'info',
  formatters: {
    level: (label) => {
      return { level: label };
    },
  },
  transport: process.env.NODE_ENV === 'development'
    ? {
        target: 'pino-pretty',
        options: {
          colorize: true,
          translateTime: 'SYS:standard',
          ignore: 'pid,hostname',
        },
      }
    : undefined,
});

export default logger;

// Usage examples
logger.info('Server started');

logger.info({
  competitorId: 'xxx',
  productsCount: 100,
  duration: 5432,
}, 'Scraping completed');

logger.error({
  error: error.message,
  stack: error.stack,
  competitorUrl: 'https://...',
}, 'Scraping failed');
```

**Avantages**:
- ‚úÖ Logs JSON facilement parsables
- ‚úÖ Contexte riche (metadata)
- ‚úÖ Pretty print en dev, JSON en prod

### 2. Error Tracking avec Sentry

**Configuration compl√®te**:

```bash
# Install Sentry
npm install @sentry/node @sentry/tracing
```

```typescript
// worker/src/utils/sentry.ts
import * as Sentry from '@sentry/node';
import * as Tracing from '@sentry/tracing';
import { Express } from 'express';

export function initSentry(app: Express) {
  Sentry.init({
    dsn: process.env.SENTRY_DSN,
    environment: process.env.NODE_ENV || 'development',

    // Performance monitoring
    tracesSampleRate: 1.0,

    // Attach Express integration
    integrations: [
      new Sentry.Integrations.Http({ tracing: true }),
      new Tracing.Integrations.Express({ app }),
    ],

    // Filter sensitive data
    beforeSend(event, hint) {
      // Remove API keys from logs
      if (event.request?.headers) {
        delete event.request.headers['x-api-key'];
        delete event.request.headers['authorization'];
      }
      return event;
    },
  });

  // Request handler (must be first middleware)
  app.use(Sentry.Handlers.requestHandler());

  // Tracing handler
  app.use(Sentry.Handlers.tracingHandler());
}

// Error handler (must be last middleware)
export function sentryErrorHandler(app: Express) {
  app.use(Sentry.Handlers.errorHandler());
}

// Usage in index.ts
import { initSentry, sentryErrorHandler } from './utils/sentry';

const app = express();
initSentry(app);

// ... routes ...

sentryErrorHandler(app);
app.listen(PORT);
```

**Features Sentry**:
- ‚úÖ Error tracking avec stack traces
- ‚úÖ Performance monitoring (transactions)
- ‚úÖ Release tracking
- ‚úÖ Source maps support
- ‚úÖ Free tier: 5,000 errors/mois

### 3. Metrics Endpoint (Prometheus)

**Objectif**: M√©triques pour monitoring (Grafana, Railway metrics).

```bash
npm install prom-client
```

```typescript
// worker/src/utils/metrics.ts
import prometheus from 'prom-client';

// Register
export const register = new prometheus.Registry();

// Default metrics (CPU, memory, etc.)
prometheus.collectDefaultMetrics({ register });

// Custom metrics
export const scrapeDuration = new prometheus.Histogram({
  name: 'scrape_duration_seconds',
  help: 'Duration of scraping jobs in seconds',
  labelNames: ['competitor', 'company_slug', 'status'],
  buckets: [1, 5, 15, 30, 60, 120, 300, 600], // seconds
  registers: [register],
});

export const scrapeTotal = new prometheus.Counter({
  name: 'scrape_total',
  help: 'Total number of scrape requests',
  labelNames: ['competitor', 'company_slug', 'status'],
  registers: [register],
});

export const productsScrapedTotal = new prometheus.Counter({
  name: 'products_scraped_total',
  help: 'Total number of products scraped',
  labelNames: ['competitor', 'company_slug', 'found'],
  registers: [register],
});

// Metrics endpoint
app.get('/metrics', async (req, res) => {
  res.set('Content-Type', register.contentType);
  res.end(await register.metrics());
});

// Usage in scraping
const end = scrapeDuration.startTimer({
  competitor: competitorName,
  company_slug: companySlug,
});

try {
  const result = await scraper.scrapeCompetitor(...);

  scrapeTotal.inc({
    competitor: competitorName,
    company_slug: companySlug,
    status: 'success',
  });

  productsScrapedTotal.inc({
    competitor: competitorName,
    company_slug: companySlug,
    found: 'true',
  }, result.productsScraped);

  end({ status: 'success' });
} catch (error) {
  scrapeTotal.inc({
    competitor: competitorName,
    company_slug: companySlug,
    status: 'error',
  });

  end({ status: 'error' });
}
```

**Railway Integration**: Railway peut scraper `/metrics` automatiquement pour monitoring.

---

## ‚ö†Ô∏è Gestion des Risques

### 1. Circuit Breaker Pattern

**Probl√®me**: Si Railway worker tombe, toutes les requ√™tes √©chouent pendant des minutes.

**Solution**: Circuit breaker qui d√©tecte les failures et stoppe temporairement les appels.

```bash
npm install opossum
```

```typescript
// src/lib/pricing/circuit-breaker.ts
import CircuitBreaker from 'opossum';

export function createWorkerCircuitBreaker(workerClient: WorkerClient) {
  const options = {
    timeout: 600000, // 10 minutes
    errorThresholdPercentage: 50, // Open circuit if 50% of requests fail
    resetTimeout: 30000, // Try again after 30 seconds
    rollingCountTimeout: 60000, // 1 minute window
    rollingCountBuckets: 10,
  };

  const breaker = new CircuitBreaker(
    async (request: ScrapeRequest) => {
      return await workerClient.scrape(request);
    },
    options
  );

  // Event handlers
  breaker.on('open', () => {
    console.error('[Circuit Breaker] OPEN - Railway worker appears down');
    // TODO: Send alert (Slack, email)
  });

  breaker.on('halfOpen', () => {
    console.log('[Circuit Breaker] HALF_OPEN - Testing Railway worker');
  });

  breaker.on('close', () => {
    console.log('[Circuit Breaker] CLOSED - Railway worker healthy');
  });

  breaker.fallback(() => {
    console.log('[Circuit Breaker] Fallback triggered');
    return {
      success: false,
      scrapedProducts: [],
      productsScraped: 0,
      productsFailed: 0,
      errors: [{
        url: 'CIRCUIT_BREAKER',
        error: 'Railway worker unavailable (circuit open)',
        timestamp: new Date().toISOString(),
      }],
      metadata: {
        duration: 0,
        scraperType: 'playwright' as const,
        workerStatus: 'DOWN' as const,
      },
    };
  });

  return breaker;
}

// Usage in ScrapingService
constructor() {
  this.matchingService = new MatchingService();
  this.workerClient = new WorkerClient();
  this.workerBreaker = createWorkerCircuitBreaker(this.workerClient);
}

private async executeScraping(...) {
  // Use circuit breaker instead of direct call
  const result = await this.workerBreaker.fire(request);
}
```

### 2. Checkpointing & Resume

**Probl√®me**: Si scraping crash apr√®s 4 heures, tout est perdu.

**Solution**: Sauvegarder √©tat interm√©diaire dans PostgreSQL.

```typescript
// Add to schema
export const pricingScrapingCheckpoints = pgTable('pricing_scraping_checkpoints', {
  id: text('id').primaryKey(),
  scanId: text('scan_id').notNull().references(() => pricingScans.id),
  competitorId: text('competitor_id').notNull(),
  lastProductIndex: integer('last_product_index').notNull(),
  scrapedProducts: jsonb('scraped_products').notNull(), // Partial results
  createdAt: timestamp('created_at').defaultNow().notNull(),
  updatedAt: timestamp('updated_at').defaultNow().notNull(),
});

// Save checkpoint every 50 products
const CHECKPOINT_INTERVAL = 50;

for (let i = startIndex; i < products.length; i++) {
  const result = await this.scrapeProduct(products[i]);
  results.push(result);

  if (i % CHECKPOINT_INTERVAL === 0) {
    await db.insert(pricingScrapingCheckpoints).values({
      id: createId(),
      scanId,
      competitorId,
      lastProductIndex: i,
      scrapedProducts: results,
    });

    console.log(`[Checkpoint] Saved progress: ${i}/${products.length}`);
  }
}

// Resume from checkpoint
async function resumeFromCheckpoint(scanId: string): Promise<number> {
  const [checkpoint] = await db
    .select()
    .from(pricingScrapingCheckpoints)
    .where(eq(pricingScrapingCheckpoints.scanId, scanId))
    .orderBy(desc(pricingScrapingCheckpoints.lastProductIndex))
    .limit(1);

  if (checkpoint) {
    console.log(`[Resume] Resuming from index ${checkpoint.lastProductIndex}`);
    return checkpoint.lastProductIndex + 1;
  }

  return 0; // Start from beginning
}
```

### 3. Timeout Management Strategy

**Probl√®me Identifi√© v2**:
- 1 concurrent = 576 produits √ó 3s = **28.8 minutes**
- Timeout WorkerClient = 10 minutes ‚ùå

**Solutions**:

**Option A: Augmenter timeout (RECOMMAND√â pour Phase 1)**
```typescript
export class WorkerClient {
  constructor() {
    this.timeout = 1800000; // 30 minutes (safe pour 576 produits)
  }
}
```

**Option B: Pagination automatique (RECOMMAND√â pour Phase 2)**
- D√©j√† impl√©ment√©e dans `WorkerClient.scrape()` (batches de 100)
- 100 produits √ó 3s = 5 minutes (bien en dessous de 10 min)

**Option C: Railway timeout configuration**
```json
// railway.json
{
  "deploy": {
    "healthcheckPath": "/health",
    "healthcheckTimeout": 300, // 5 minutes
    "restartPolicyType": "ON_FAILURE",
    "sleepAfter": "30m" // Keep alive 30 minutes
  }
}
```

**Recommandation**: Combiner Option A + B pour maximum fiabilit√©.

---

## üîÑ Flux de Donn√©es (CORRIG√â v2)

### S√©quence Compl√®te: User Click ‚Üí Results Displayed

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. User clicks "Lancer scan" button                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. Frontend calls POST /api/companies/dissan/pricing/scans         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. ScrapingService.scrapeAllCompetitors(companyId)                 ‚îÇ
‚îÇ    - Fetches active competitors from DB (13 competitors)            ‚îÇ
‚îÇ    - SEQUENTIAL loop: for (competitor of competitors)               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 4. LOOP ITERATION 1: Scrape Swish                                  ‚îÇ
‚îÇ    ScrapingService.scrapeCompetitor(swishId)                        ‚îÇ
‚îÇ    - Fetches 576 active products                                    ‚îÇ
‚îÇ    - Calls executeScraping(swish, scanId, logs)                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 5. executeScraping() calls WorkerClient.scrape()                   ‚îÇ
‚îÇ    Request: {                                                        ‚îÇ
‚îÇ      competitorId: 'swish-id',                                      ‚îÇ
‚îÇ      products: [576 products],                                      ‚îÇ
‚îÇ      companySlug: 'dissan'                                          ‚îÇ
‚îÇ    }                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 6. WorkerClient detects 576 products > 100                         ‚îÇ
‚îÇ    - Splits into 6 batches (100 each)                               ‚îÇ
‚îÇ    - Batch 1: products[0..99]                                       ‚îÇ
‚îÇ    - Batch 2: products[100..199]                                    ‚îÇ
‚îÇ    - ... (6 batches total)                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 7. BATCH 1: POST to Railway Worker                                 ‚îÇ
‚îÇ    POST https://worker.railway.app/api/scrape                       ‚îÇ
‚îÇ    Body: { competitorId, products[0..99], batchInfo: {1/6} }        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 8. Railway Worker receives batch 1                                 ‚îÇ
‚îÇ    - Validates request (Zod)                                        ‚îÇ
‚îÇ    - ScraperFactory.getScraperForCompany('dissan')                  ‚îÇ
‚îÇ    - Returns DissanScraper instance                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 9. DissanScraper.scrapeCompetitor(batch 1)                         ‚îÇ
‚îÇ    - Launches Playwright browser                                    ‚îÇ
‚îÇ    - For product in products[0..99]:                                ‚îÇ
‚îÇ      ‚Ä¢ Navigate to swish.ca/search?q={sku}                          ‚îÇ
‚îÇ      ‚Ä¢ Extract price, name, image                                   ‚îÇ
‚îÇ      ‚Ä¢ Add to scrapedProducts[]                                     ‚îÇ
‚îÇ    - Close browser                                                  ‚îÇ
‚îÇ    Duration: ~5 minutes (100 √ó 3s)                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 10. Railway responds batch 1                                       ‚îÇ
‚îÇ     { success: true, scrapedProducts: [87], errors: [13] }          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 11. WorkerClient processes batch 1 response                        ‚îÇ
‚îÇ     - Aggregates results                                             ‚îÇ
‚îÇ     - Proceeds to batch 2...                                        ‚îÇ
‚îÇ     (Repeat steps 7-11 for batches 2-6)                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 12. All 6 batches completed (Swish done)                           ‚îÇ
‚îÇ     Total duration: ~30 minutes                                      ‚îÇ
‚îÇ     Aggregated: { scrapedProducts: [485], errors: [91] }            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 13. MatchingService.matchProducts(swishProducts)                   ‚îÇ
‚îÇ     - Calls GPT-5 with your catalog + Swish products                ‚îÇ
‚îÇ     - Returns matches with confidence scores                         ‚îÇ
‚îÇ     - Saves to pricing_matches table                                 ‚îÇ
‚îÇ     Duration: ~2 minutes                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 14. ScrapingService updates pricingScans                           ‚îÇ
‚îÇ     - status: 'completed' for Swish                                  ‚îÇ
‚îÇ     - productsScraped: 485, productsMatched: 123                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 15. LOOP ITERATION 2: Scrape Grainger                              ‚îÇ
‚îÇ     (Repeat steps 4-14 for Grainger)                                ‚îÇ
‚îÇ     Duration: ~30 minutes                                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ... LOOP ITERATIONS 3-13 ...                                       ‚îÇ
‚îÇ     (Uline, Amazon, Staples, etc.)                                  ‚îÇ
‚îÇ     Total: 13 competitors √ó 30 min = 6.5 hours                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 16. scrapeAllCompetitors() completes                               ‚îÇ
‚îÇ     Returns: { totalCompetitors: 13, successfulScans: 12, failed: 1}‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 17. Frontend receives success response                             ‚îÇ
‚îÇ     - Shows alert "Scan lanc√© avec succ√®s!"                          ‚îÇ
‚îÇ     - Refreshes matches via GET /api/pricing/matches                 ‚îÇ
‚îÇ     - Displays 12 competitor cards with prices                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Temps estim√© total** (CORRIG√â v2):
- 100 produits √ó 1 concurrent: ~5 minutes (scraping) + 2 min (matching) = **7 min**
- 576 produits √ó 1 concurrent: ~30 minutes (scraping) + 2 min (matching) = **32 min**
- 576 produits √ó 13 concurrents: 32 min √ó 13 = **6.9 heures**

---

## üöÄ Impl√©mentation Technique

### Phase 0: Pr√©paration Database (0.5 heure)

#### √âtape 0.1: Cr√©er migration SQL

```bash
touch migrations/0015_add_company_slug_to_competitors.sql
```

Voir section "Phase 0: Pr√©paration Database" pour le contenu SQL.

#### √âtape 0.2: Cr√©er script Node.js

```bash
touch scripts/run-migration-0015.mjs
```

Voir section "Phase 0" pour le code.

#### √âtape 0.3: Ex√©cuter migration

```bash
# Dev
node scripts/run-migration-0015.mjs

# V√©rifier
psql $DATABASE_URL -c "SELECT id, name, company_slug FROM pricing_competitors LIMIT 5;"
```

**‚úÖ Phase 0 compl√©t√©e**: Database pr√™te avec `company_slug`.

---

### Phase 1: Setup Railway Worker (3 heures)

#### √âtape 1.1: Cr√©er structure worker

```bash
mkdir -p worker/src/{scrapers,types,utils,middleware}
cd worker
npm init -y
```

#### √âtape 1.2: Installer d√©pendances

```bash
npm install express playwright zod dotenv pino @sentry/node prom-client
npm install -D @types/express @types/node typescript tsx nodemon
npm install express-rate-limit # Security
```

#### √âtape 1.3: package.json

```json
{
  "name": "pricing-worker",
  "version": "2.0.0",
  "scripts": {
    "dev": "nodemon --watch src --exec tsx src/index.ts",
    "build": "tsc",
    "start": "node dist/index.js",
    "test": "echo \"No tests yet\""
  },
  "dependencies": {
    "express": "^4.18.2",
    "playwright": "^1.40.0",
    "zod": "^3.22.4",
    "dotenv": "^16.3.1",
    "pino": "^8.16.0",
    "pino-pretty": "^10.2.0",
    "@sentry/node": "^7.80.0",
    "prom-client": "^15.0.0",
    "express-rate-limit": "^7.1.0"
  },
  "devDependencies": {
    "@types/express": "^4.17.21",
    "@types/node": "^20.10.0",
    "typescript": "^5.3.0",
    "tsx": "^4.7.0",
    "nodemon": "^3.0.2"
  }
}
```

#### √âtape 1.4: tsconfig.json

```json
{
  "compilerOptions": {
    "target": "ES2022",
    "module": "commonjs",
    "lib": ["ES2022"],
    "outDir": "./dist",
    "rootDir": "./src",
    "strict": true,
    "esModuleInterop": true,
    "skipLibCheck": true,
    "forceConsistentCasingInFileNames": true,
    "resolveJsonModule": true,
    "declaration": true,
    "declarationMap": true,
    "sourceMap": true
  },
  "include": ["src/**/*"],
  "exclude": ["node_modules", "dist"]
}
```

#### √âtape 1.5: Copier code Dissan

```bash
# Copier les scrapers Dissan dans worker
cp -r ../Dissan/price-scraper/src/scrapers/* worker/src/scrapers/dissan/
cp -r ../Dissan/price-scraper/src/matchers worker/src/matchers/
cp -r ../Dissan/price-scraper/src/utils worker/src/utils/

# Adapter les imports si n√©cessaire
```

#### √âtape 1.6: Cr√©er fichiers types

```bash
touch worker/src/types/index.ts
touch worker/src/types/scraper-interface.ts
```

```typescript
// worker/src/types/scraper-interface.ts
export interface ICompetitorScraper {
  scraperType: 'playwright' | 'apify' | 'api';

  scrapeCompetitor(request: ScrapeCompetitorRequest): Promise<ScrapeCompetitorResponse>;
}

export interface ScrapeCompetitorRequest {
  competitorId: string;
  competitorName: string;
  competitorUrl: string;
  products: Array<{
    id: string;
    sku: string;
    name: string;
    brand: string | null;
    category: string | null;
  }>;
}

export interface ScrapeCompetitorResponse {
  scrapedProducts: Array<{
    url: string;
    name: string;
    sku?: string;
    price: number;
    currency: string;
    inStock: boolean;
    imageUrl?: string;
    characteristics?: Record<string, any>;
  }>;
  productsScraped: number;
  productsFailed: number;
  errors: Array<{
    url: string;
    error: string;
    timestamp: Date;
  }>;
}
```

#### √âtape 1.7: Cr√©er index.ts

Voir section "Composants D√©taill√©s - Railway Worker" pour le code complet.

#### √âtape 1.8: Tester localement

```bash
cd worker
npm run dev

# Test health check
curl http://localhost:3001/health
```

---

### Phase 1.5: Batching Logic (1 heure)

D√©j√† impl√©ment√© dans le code ci-dessus:
- ‚úÖ `WorkerClient.scrape()` avec pagination automatique
- ‚úÖ `ScrapingService.scrapeAllCompetitors()` avec loop s√©quentiel

**Tests √† effectuer**:

```bash
# Test 1: Small batch (10 products)
curl -X POST http://localhost:3001/api/scrape \
  -H "Content-Type: application/json" \
  -H "X-API-Key: test-key" \
  -d '{
    "companySlug": "dissan",
    "competitorId": "test",
    "competitorName": "Test Competitor",
    "competitorUrl": "https://example.com",
    "products": [...10 products...]
  }'

# Test 2: Large batch (150 products) - should auto-paginate
curl -X POST http://localhost:3001/api/scrape \
  -H "Content-Type: application/json" \
  -H "X-API-Key: test-key" \
  -d '{
    "companySlug": "dissan",
    "products": [...150 products...]
  }'
# Should see logs: "Paginating 150 products into batches of 100"
```

---

## üì¶ D√©ploiement Railway

### Phase 2: D√©ployer sur Railway (1 heure)

#### √âtape 2.1: Installer Railway CLI

```bash
npm i -g @railway/cli
railway login
```

#### √âtape 2.2: Cr√©er projet Railway

```bash
cd worker
railway init
# Choose: "Create new project"
# Name: "pricing-worker-dissan"

railway up
```

#### √âtape 2.3: Configurer variables d'environnement

```bash
# Generate secure API key
openssl rand -hex 32
# Output: a1b2c3d4e5f6...

railway variables set API_KEY=a1b2c3d4e5f6...
railway variables set NODE_ENV=production
railway variables set LOG_LEVEL=info

# Sentry (optional Phase 3)
railway variables set SENTRY_DSN=https://...@sentry.io/...

# Database (if needed for checkpointing)
railway variables set DATABASE_URL=${{Postgres.DATABASE_URL}}
```

#### √âtape 2.4: railway.json

```json
{
  "$schema": "https://railway.app/railway.schema.json",
  "build": {
    "builder": "NIXPACKS",
    "buildCommand": "npm install && npx playwright install --with-deps chromium && npm run build"
  },
  "deploy": {
    "numReplicas": 1,
    "sleepAfter": "30m",
    "restartPolicyType": "ON_FAILURE",
    "startCommand": "npm start",
    "healthcheckPath": "/health",
    "healthcheckTimeout": 30
  }
}
```

#### √âtape 2.5: Obtenir URL Railway

```bash
railway domain
# Output: https://pricing-worker-dissan-production.up.railway.app
```

#### √âtape 2.6: Tester d√©ploiement

```bash
curl https://pricing-worker-dissan-production.up.railway.app/health \
  -H "X-API-Key: a1b2c3d4e5f6..."

# Expected response:
# {"status":"healthy","timestamp":"2025-01-19T...","uptime":123.45}
```

---

### Phase 2.5: Modifier Next.js (2 heures)

#### √âtape 2.5.1: Cr√©er WorkerClient

```bash
touch src/lib/pricing/worker-client.ts
```

Copier le code de la section "Composants D√©taill√©s - WorkerClient".

#### √âtape 2.5.2: Modifier ScrapingService

√âditer `src/lib/pricing/scraping-service.ts`:
- Importer `WorkerClient`
- Modifier `scrapeAllCompetitors()` (loop s√©quentiel)
- Modifier `executeScraping()` (appel Railway)

Voir code complet dans section "Composants D√©taill√©s - ScrapingService".

#### √âtape 2.5.3: Ajouter variables d'environnement

```bash
# .env.local (dev)
RAILWAY_WORKER_URL=http://localhost:3001
RAILWAY_WORKER_API_KEY=test-key

# Production (Vercel)
# Settings ‚Üí Environment Variables
RAILWAY_WORKER_URL=https://pricing-worker-dissan-production.up.railway.app
RAILWAY_WORKER_API_KEY=a1b2c3d4e5f6...
```

Dans Vercel dashboard:
1. Settings ‚Üí Environment Variables
2. Add: `RAILWAY_WORKER_URL` = `https://pricing-worker-dissan-production.up.railway.app`
3. Add: `RAILWAY_WORKER_API_KEY` = `a1b2c3d4e5f6...`
4. Redeploy

#### √âtape 2.5.4: Tester en local

```bash
# Terminal 1: Railway worker
cd worker
npm run dev

# Terminal 2: Next.js
npm run dev

# Browser: http://localhost:3000/companies/dissan/pricing
# Click "Lancer scan" button
# Check logs in both terminals
```

---

## üß™ Tests et Validation

### Phase 3: Tests (2 heures)

#### Test 1: Health Check Railway

```bash
curl -H "X-API-Key: a1b2c3d4e5f6..." \
  https://pricing-worker-dissan-production.up.railway.app/health

# Expected: {"status":"healthy",...}
```

#### Test 2: Scrape 10 produits

```bash
# Create test data
cat > test-10-products.json <<EOF
{
  "companyId": "test-company",
  "companySlug": "dissan",
  "competitorId": "test-competitor",
  "competitorName": "Swish Test",
  "competitorUrl": "https://swish.ca",
  "products": [
    {"id": "1", "sku": "TEST-001", "name": "Test Product 1", "brand": "Test", "category": "Test"},
    ...
  ]
}
EOF

curl -X POST \
  -H "Content-Type: application/json" \
  -H "X-API-Key: a1b2c3d4e5f6..." \
  https://pricing-worker-dissan-production.up.railway.app/api/scrape \
  -d @test-10-products.json

# Check response time (should be <1 minute for 10 products)
```

#### Test 3: Scrape 150 produits (pagination)

```bash
# Create 150 products test data
# ... (similar to above, 150 items)

curl -X POST ... -d @test-150-products.json

# Check logs for pagination:
# "Paginating 150 products into batches of 100"
# "Processing batch 1/2"
# "Processing batch 2/2"
```

#### Test 4: End-to-end depuis UI

1. Login: http://localhost:3000 (ou production)
2. Navigate: `/companies/dissan/pricing`
3. Click: "Lancer scan"
4. Monitor:
   - Railway logs: `railway logs --tail`
   - Next.js terminal
   - Browser network tab
5. Verify:
   - Scan record cr√©√© dans `pricing_scans`
   - Matches cr√©√©es dans `pricing_matches`
   - UI affiche les r√©sultats

#### Test 5: Error Handling

```bash
# Test 5.1: Invalid API Key
curl -X POST ... -H "X-API-Key: wrong-key"
# Expected: 401 Unauthorized

# Test 5.2: Invalid request (missing field)
curl -X POST ... -d '{"invalid": "data"}'
# Expected: 400 Bad Request, Zod validation errors

# Test 5.3: Timeout simulation
# Modify worker to sleep 15 minutes
# Expected: WorkerClient retries, then fallback

# Test 5.4: Railway worker down
# Stop Railway worker
# Expected: Circuit breaker opens, fallback response
```

#### Test 6: Performance Test

```bash
# Measure duration for 576 products
time curl -X POST ... -d @dissan-576-products.json

# Expected: ~30 minutes (576 products √ó 3s/product)
```

---

## üí∞ Co√ªts et Performance (R√âVIS√â v2)

### Co√ªts Railway

**Free Tier**: $5 cr√©dit/mois

**Pricing Model Railway** (2024):
- Hobby Plan: $0.000463/minute de runtime
- Includes: 500MB RAM, 1 vCPU
- Auto-sleep apr√®s 30 minutes d'inactivit√©

**Estimation Dissan** (CORRIG√â v2):

```
Hypoth√®ses:
- 576 produits dans votre catalogue
- 13 concurrents actifs
- Scraping: 3 secondes/produit (moyenne)
- Fr√©quence: 1 scan complet par semaine

Calcul par scan:
1. Scraping d'un concurrent:
   - 576 produits √ó 3s = 1,728s = 28.8 minutes
   - Pagination: 6 batches √ó 5 min = 30 minutes
   - Total par concurrent: ~30 minutes

2. Scraping de tous les concurrents (s√©quentiel):
   - 13 concurrents √ó 30 min = 390 minutes = 6.5 heures

3. Co√ªt par scan:
   - 390 min √ó $0.000463 = $0.18/scan

4. Co√ªt mensuel:
   - 4 scans/mois √ó $0.18 = $0.72/mois

Avec $5 gratuit: 5 / 0.72 = 6.9 mois d'utilisation ‚úÖ
```

**Comparaison v1 vs v2**:
- ‚ùå **v1**: $0.06/mois (calcul incorrect, assumait parallel)
- ‚úÖ **v2**: $0.72/mois (calcul r√©aliste, scraping s√©quentiel)

**Scaling pour d'autres clients**:
```
Akonovia (hypoth√®se: 200 produits, 5 concurrents):
- 200 √ó 3s √ó 5 = 3,000s = 50 minutes/scan
- 4 scans/mois = 200 min/mois
- 200 min √ó $0.000463 = $0.09/mois

Total (Dissan + Akonovia):
- $0.72 + $0.09 = $0.81/mois
- Avec $5 gratuit = 6 mois d'utilisation ‚úÖ
```

**Note**: Railway auto-sleep r√©duit co√ªts si pas d'activit√©.

---

### Performance

**Scraping**:
- 10 produits: ~30 secondes
- 100 produits: ~5 minutes
- 576 produits (Dissan): ~30 minutes
- 576 √ó 13 concurrents: ~6.5 heures

**Matching GPT-5**:
- 10 produits: ~10 secondes
- 100 produits: ~2 minutes
- 576 produits: ~2-3 minutes (batches de 10)

**Total End-to-End** (Dissan complet):
- Scraping: 6.5 heures
- Matching: 13 √ó 2 min = 26 minutes
- **Total: ~7 heures**

**Optimisations futures** (Phase 4):
- Parallel browsers (3 concurrent): 6.5h ‚Üí 2.2h
- Caching intelligent: R√©duire scraping r√©p√©t√©
- Incremental updates: Scraper seulement produits nouveaux/modifi√©s

---

## üõ£Ô∏è Roadmap (R√âVIS√âE v2)

### Phase 0: Pr√©paration Database ‚úÖ (0.5 heure)
- [x] Migration SQL: Add `company_slug` to `pricing_competitors`
- [x] Script Node.js pour migration
- [x] Tests et validation

**Livrables**: Database schema updated, migration tested.

---

### Phase 1: MVP Railway Worker (3 heures)
- [x] Setup projet worker (`/worker`)
- [x] Install dependencies (Express, Playwright, Zod, Pino)
- [x] Create Express server avec middleware
- [x] Copy Dissan scraper code
- [x] Create ScraperFactory
- [x] Test localement (`npm run dev`)

**Livrables**: Worker fonctionnel en local.

---

### Phase 1.5: Batching Logic (1 heure)
- [x] Implement `WorkerClient` avec pagination
- [x] Modify `ScrapingService.scrapeAllCompetitors()` (loop s√©quentiel)
- [x] Test avec 10, 100, 150 produits
- [x] Valider timeouts

**Livrables**: Batching logic valid√©.

---

### Phase 2: D√©ploiement Production (2 heures)
- [ ] Deploy Railway worker (`railway up`)
- [ ] Configure env vars (API_KEY, NODE_ENV)
- [ ] Obtenir URL Railway
- [ ] Modifier Next.js (WorkerClient, ScrapingService)
- [ ] Configure Vercel env vars
- [ ] Deploy Vercel
- [ ] Test end-to-end production

**Livrables**: Production deployed, end-to-end fonctionnel.

---

### Phase 3: Monitoring & Tests (2 heures)
- [ ] Setup Sentry error tracking
- [ ] Structured logging (Pino)
- [ ] Metrics endpoint (`/metrics`)
- [ ] Test suite:
  - Health check
  - 10 produits
  - 150 produits (pagination)
  - Error scenarios
- [ ] Performance benchmarks

**Livrables**: Monitoring actif, tests passants.

---

### Phase 4: S√©curit√© & UX (3 heures)
- [ ] Rate limiting (express-rate-limit)
- [ ] IP whitelist (Vercel IPs)
- [ ] Progress polling (UI)
  - Endpoint: `GET /api/scans/{scanId}`
  - Frontend: Poll toutes les 5s
  - Display: Progress bar + ETA
- [ ] Circuit breaker (opossum)

**Livrables**: S√©curit√© renforc√©e, UX am√©lior√©e.

---

### Phase 5: Robustesse & Recovery (4 heures)
- [ ] Checkpointing system (PostgreSQL)
  - Table: `pricing_scraping_checkpoints`
  - Save every 50 products
- [ ] Resume logic
- [ ] Retry avec exponential backoff
- [ ] Error recovery tests

**Livrables**: System r√©silient aux failures.

---

### Phase 6: Multi-Tenant & Scaling (Futur)
- [ ] Table `pricing_scraper_configs` (config DB)
- [ ] ScraperFactory dynamique (config-driven)
- [ ] Akonovia scraper (client 2)
- [ ] Admin UI pour configurer scrapers
- [ ] Queue system (BullMQ)
- [ ] Parallel scraping (multiple browsers)

**Livrables**: Fully multi-tenant, scalable.

---

**Total Temps MVP (Phases 0-3)**: ~8 heures
**Total Temps Production-Ready (Phases 0-4)**: ~14 heures
**Total Temps Fully Featured (Phases 0-5)**: ~18 heures

---

## ‚úÖ Checklist Impl√©mentation

### Phase 0: Database
- [ ] Cr√©er migration SQL (`0015_add_company_slug_to_competitors.sql`)
- [ ] Cr√©er script Node.js (`run-migration-0015.mjs`)
- [ ] Ex√©cuter migration en dev
- [ ] V√©rifier donn√©es (tous les rows ont `company_slug`)
- [ ] Tester query avec JOIN

### Phase 1: Railway Worker
- [ ] Cr√©er r√©pertoire `/worker`
- [ ] `npm init` + installer d√©pendances
- [ ] Cr√©er `tsconfig.json`
- [ ] Cr√©er `src/index.ts` (Express server)
- [ ] Cr√©er `src/types/scraper-interface.ts`
- [ ] Cr√©er `src/scrapers/factory.ts`
- [ ] Copier code Dissan dans `src/scrapers/dissan/`
- [ ] Adapter imports Dissan si n√©cessaire
- [ ] Cr√©er `.env` avec API_KEY
- [ ] Tester localement (`npm run dev`)
- [ ] Test health check (`curl /health`)
- [ ] Test scraping mock data

### Phase 1.5: Batching
- [ ] Implement pagination dans `WorkerClient`
- [ ] Modify `scrapeAllCompetitors()` (sequential)
- [ ] Test avec 10 produits
- [ ] Test avec 150 produits (pagination)
- [ ] Valider logs "Paginating..."

### Phase 2: Deployment
- [ ] Install Railway CLI (`npm i -g @railway/cli`)
- [ ] `railway init`
- [ ] Create `railway.json` config
- [ ] `railway up` (first deploy)
- [ ] Configure env vars Railway
- [ ] Test deployed health check
- [ ] Cr√©er `src/lib/pricing/worker-client.ts` (Next.js)
- [ ] Modifier `src/lib/pricing/scraping-service.ts`
- [ ] Add env vars Vercel dashboard
- [ ] Deploy Vercel
- [ ] Test end-to-end production

### Phase 3: Monitoring
- [ ] Install Sentry (`npm install @sentry/node`)
- [ ] Configure Sentry DSN (Railway env var)
- [ ] Add Sentry middleware to Express
- [ ] Install Pino (`npm install pino pino-pretty`)
- [ ] Replace `console.log` with `logger.info`
- [ ] Create `/metrics` endpoint
- [ ] Test error reporting (trigger error)
- [ ] Test logs in Railway dashboard

### Phase 4: Security & UX
- [ ] Install rate-limit (`npm install express-rate-limit`)
- [ ] Add rate limiter middleware
- [ ] Test rate limiting (exceed 100 req/15min)
- [ ] Implement IP whitelist (Vercel IPs)
- [ ] Create progress polling endpoint
- [ ] Frontend: Poll `/api/scans/{scanId}` every 5s
- [ ] Display progress bar + ETA
- [ ] Install opossum (`npm install opossum`)
- [ ] Implement circuit breaker
- [ ] Test circuit breaker (kill Railway worker)

### Phase 5: Recovery
- [ ] Create `pricing_scraping_checkpoints` table
- [ ] Implement checkpoint save (every 50 products)
- [ ] Implement resume logic
- [ ] Test: Kill scraping mid-process, resume
- [ ] Implement retry with exponential backoff
- [ ] Test retry logic

---

## üéì Conclusion

Cette architecture v2 permet:

‚úÖ **Playwright fonctionne** - Plus de limitations Vercel
‚úÖ **Code Dissan r√©utilis√©** - 13 scrapers d√©j√† test√©s
‚úÖ **Multi-tenant ready** - Facile d'ajouter nouveaux clients
‚úÖ **Interface unifi√©e** - UI/API inchang√©es
‚úÖ **Co√ªt minimal** - $0.72/mois pour Dissan (dans $5 gratuit)
‚úÖ **Scalable** - Railway peut scaler automatiquement
‚úÖ **Monitoring int√©gr√©** - Sentry + Pino + Metrics
‚úÖ **S√©curit√© renforc√©e** - Rate limiting + IP whitelist
‚úÖ **R√©silient** - Circuit breaker + Checkpointing + Retry

### Diff√©rences Critiques v1 ‚Üí v2

| Aspect | v1 | v2 |
|--------|----|----|
| **Batching** | ‚ùå 1 call = ALL competitors | ‚úÖ 1 call = 1 competitor |
| **Pagination** | ‚ùå Pas de pagination | ‚úÖ Auto-pagination >100 products |
| **Database** | ‚ùå Manque company_slug | ‚úÖ Phase 0: Migration SQL |
| **Monitoring** | ‚ùå Pas de monitoring | ‚úÖ Sentry + Pino + Metrics |
| **S√©curit√©** | ‚ùå API Key seulement | ‚úÖ Rate limiting + IP whitelist |
| **Recovery** | ‚ùå Pas de checkpointing | ‚úÖ Checkpoints + Resume |
| **Co√ªts** | ‚ùå $0.06/mois (faux) | ‚úÖ $0.72/mois (r√©aliste) |
| **Roadmap** | ‚ùå 4 phases | ‚úÖ 6 phases (+ Phase 0, 1.5) |

---

## üìö Ressources

- Railway Docs: https://docs.railway.app
- Playwright Docs: https://playwright.dev
- Sentry Node.js: https://docs.sentry.io/platforms/node/
- Pino Logging: https://github.com/pinojs/pino
- Opossum Circuit Breaker: https://github.com/nodeshift/opossum

---

**Next Step**: Impl√©menter Phase 0 (Migration Database) puis Phase 1 (Setup Railway Worker)

**Contact**: Pour questions ou clarifications sur cette architecture, r√©f√©rer √† ce document (v2).

**Version History**:
- v1.0 (2025-01-19): Version initiale
- v2.0 (2025-01-19): Corrections critiques post-revue architecturale
