# Guide - Prochaines Ã‰tapes

**Date:** 18 novembre 2024
**Statut:** âœ… Phase 3 complÃ©tÃ©e - Scrapers pilotes opÃ©rationnels

---

## âœ… Ce qui a Ã©tÃ© accompli

### Infrastructure ComplÃ¨te (100%)
- âœ… Filtrage et prÃ©paration de 576 produits commerciaux
- âœ… Configuration de 13 compÃ©titeurs canadiens
- âœ… Architecture modulaire de scraping (BaseScraper)
- âœ… SystÃ¨me de rate limiting (2-3s entre requÃªtes)
- âœ… SystÃ¨me de checkpoints (sauvegarde tous les 50 produits)
- âœ… SystÃ¨me de logging (console + fichiers)
- âœ… Matchers intelligents (SKU + Nom avec similaritÃ©)

### 3 Scrapers Pilotes ImplÃ©mentÃ©s (100%)
1. **SwishScraper** - Swish Maintenance (swish.ca)
2. **GraingerScraper** - Grainger Canada (grainger.ca)
3. **CleanItSupplyScraper** - CleanItSupply (cleanitsupply.ca)

### Tests de Validation
```
âœ… All tests passed! Setup is ready.

ğŸ“Š Statistiques:
- 576 produits commerciaux chargÃ©s
- 13 compÃ©titeurs configurÃ©s
- 3 scrapers opÃ©rationnels
- Top marque: ATL (77 produits)
```

---

## ğŸ¯ Prochaines Ã‰tapes Critiques

### Ã‰TAPE 1: Ajuster les SÃ©lecteurs CSS (URGENT - 2-3h)

âš ï¸ **IMPORTANT:** Les sÃ©lecteurs CSS dans `competitors-config.json` sont des **placeholders fictifs**. Ils doivent Ãªtre ajustÃ©s en inspectant les vrais sites web.

#### Pour chaque site (Swish, Grainger, CleanItSupply):

1. **Ouvrir le site dans le navigateur**
   ```
   https://swish.ca
   https://grainger.ca
   https://cleanitsupply.ca
   ```

2. **Faire une recherche test** (ex: "ATL-12600")

3. **Inspecter avec DevTools (F12)** et identifier les sÃ©lecteurs pour:
   - `searchBox` - Champ de recherche (ex: `#search-input`)
   - `searchButton` - Bouton de recherche
   - `productList` - Container des rÃ©sultats (ex: `.product-grid .product-item`)
   - `productLink` - Lien vers page produit (ex: `a.product-link`)
   - `productName` - Nom du produit (ex: `.product-title`)
   - `productSku` - SKU affichÃ© (ex: `.product-sku`)
   - `productPrice` - Prix (ex: `.product-price .price-value`)
   - `noResults` - Message "aucun rÃ©sultat" (ex: `.no-results-message`)

4. **Mettre Ã  jour** `Dissan/competitors-config.json`:
   ```json
   {
     "id": "swish",
     "selectors": {
       "searchBox": "#VRAI_SELECTEUR",
       "productList": ".VRAI_CONTAINER .product-item",
       ...
     }
   }
   ```

5. **Tester immÃ©diatement:**
   ```bash
   cd Dissan/price-scraper
   npm run scrape:site swish
   ```

6. **RÃ©pÃ©ter** pour Grainger et CleanItSupply

#### Exemple Pratique (Swish):

```bash
# 1. Ouvrir DevTools sur swish.ca
# 2. Chercher "Rubbermaid bucket"
# 3. Clic droit sur champ de recherche â†’ Inspecter
#    â†’ Noter le sÃ©lecteur (ex: input.search-field)
# 4. Clic droit sur un produit â†’ Inspecter
#    â†’ Noter .product-card, .product-title, etc.
# 5. Mettre Ã  jour competitors-config.json
# 6. Tester: npm run scrape:site swish
```

---

### Ã‰TAPE 2: Tester sur Ã‰chantillon de 50 Produits (1-2h)

Une fois les sÃ©lecteurs ajustÃ©s pour les 3 sites pilotes:

```bash
cd Dissan/price-scraper

# Modifier main.ts pour tester sur 50 produits
# (Le script test n'est pas encore configurÃ© correctement)

# Option 1: Test manuel par site
npm run scrape:site swish
# VÃ©rifier les rÃ©sultats dans results/prix-par-site/swish-results.json

npm run scrape:site grainger
npm run scrape:site cleanitsupply
```

#### Validation des RÃ©sultats

VÃ©rifier dans `results/prix-par-site/{site}-results.json`:

```json
{
  "competitorId": "swish",
  "productsFound": 35,        // Objectif: >40/50 (80%)
  "productsNotFound": 12,
  "errors": 3,                // Objectif: <3/50 (5%)
  "results": [...]
}
```

**MÃ©triques de succÃ¨s visÃ©es:**
- Taux trouvÃ© (SKU + Nom): **>80%**
- Taux d'erreur: **<5%**
- Temps moyen/produit: **<10s**

Si les mÃ©triques ne sont pas atteintes:
- Ajuster les sÃ©lecteurs CSS
- Augmenter le rate limiting (3-4s si 403 errors)
- VÃ©rifier les logs: `data/logs/{site}-{date}.log`

---

### Ã‰TAPE 3: ImplÃ©menter les 10 Scrapers Restants (6-8h)

**Template disponible:** Les 3 scrapers pilotes servent de template rÃ©utilisable.

#### PrioritÃ© 1 (3 sites - 2h)
3. **ULINE Canada** (uline.ca)
4. **Bunzl Cleaning** (bunzlch.ca)
5. **Imperial Dade** (imperialdade.com)

**Processus par scraper:**
1. Copier `swish-scraper.ts` â†’ `uline-scraper.ts`
2. Renommer la classe: `UlineScraper`
3. Inspecter le site ULINE avec DevTools
4. Ajuster les sÃ©lecteurs dans `competitors-config.json`
5. Tester: `npm run scrape:site uline`
6. Ajouter au registre dans `main.ts`:
   ```typescript
   import { UlineScraper } from './scrapers/uline-scraper';

   const SCRAPERS = {
     ...
     uline: UlineScraper,
   };
   ```

#### PrioritÃ© 2 (5 sites - 3h)
6. United Canada
7. NexDay Supply
8. Clean Spot
9. Checkers Cleaning
10. *(CleanItSupply dÃ©jÃ  fait)*

#### PrioritÃ© 3 (3 sites - 2h)
11. V-TO inc.
12. Lalema Express
13. SaniDÃ©pÃ´t QuÃ©bec

---

### Ã‰TAPE 4: CrÃ©er le SystÃ¨me de Consolidation Excel (3-4h)

Une fois tous les scrapers opÃ©rationnels, crÃ©er:

#### 4.1 Excel Exporter

**Fichier:** `src/exporters/excel-exporter.ts`

```typescript
import ExcelJS from 'exceljs';

export class ExcelExporter {
  async exportConsolidated(
    products: ConsolidatedProduct[],
    outputFile: string
  ): Promise<void> {
    const workbook = new ExcelJS.Workbook();

    // Onglet 1: Tous les produits
    const sheet = workbook.addWorksheet('Tous les produits');
    sheet.columns = [
      { header: 'SKU', key: 'sku', width: 25 },
      { header: 'Nom', key: 'name', width: 50 },
      { header: 'Marque', key: 'brand', width: 20 },
      { header: 'Prix Swish', key: 'price_swish', width: 12 },
      { header: 'URL Swish', key: 'url_swish', width: 50 },
      // ... pour chaque compÃ©titeur
      { header: 'Prix Min', key: 'price_min', width: 12 },
      { header: 'Prix Max', key: 'price_max', width: 12 },
      { header: 'Prix Moyen', key: 'price_avg', width: 12 },
    ];

    // Ajouter donnÃ©es...
    products.forEach(p => sheet.addRow({...}));

    // Formatage...
    sheet.getRow(1).font = { bold: true };

    await workbook.xlsx.writeFile(outputFile);
  }
}
```

#### 4.2 Script de Consolidation

**Fichier:** `scripts/consolidate-prices.ts`

```typescript
// 1. Charger les 13 fichiers JSON de rÃ©sultats
// 2. Merger par SKU
// 3. Calculer statistiques (min, max, avg)
// 4. Exporter vers Excel
```

**Commandes:**
```bash
npm run analyze  # Lance consolidation
```

**Fichiers gÃ©nÃ©rÃ©s:**
- `Dissan/prix-competiteurs-final.xlsx` - Base de donnÃ©es complÃ¨te
- `Dissan/rapport-prix-competiteurs.xlsx` - Analyses + graphiques

---

### Ã‰TAPE 5: ExÃ©cution ComplÃ¨te (30-40h automatique)

Une fois tout opÃ©rationnel:

```bash
cd Dissan/price-scraper

# Lancer scraping complet (peut tourner la nuit/weekend)
npm run scrape:all

# OU par prioritÃ©:
npm run scrape:priority1  # ~10h (5 sites Ã— 2h)
npm run scrape:priority2  # ~15h (5 sites Ã— 3h)
npm run scrape:priority3  # ~10h (3 sites Ã— 3h)
```

**Monitoring pendant l'exÃ©cution:**
- Logs: `tail -f data/logs/swish-{date}.log`
- Checkpoints: `ls -lh data/checkpoints/`
- RÃ©sultats partiels: `ls -lh ../results/prix-par-site/`

**En cas d'interruption:**
Le scraper reprendra automatiquement au dernier checkpoint (tous les 50 produits).

---

## ğŸ“ Structure Actuelle

```
Dissan/
â”œâ”€â”€ produits-sanidepot.xlsx              # Source (890 produits)
â”œâ”€â”€ produits-commerciaux.xlsx            # FiltrÃ©s (576 produits) âœ…
â”œâ”€â”€ competitors-config.json              # Config 13 sites âš ï¸ AJUSTER SÃ‰LECTEURS
â”œâ”€â”€ PLAN-ANALYSE-PRIX-COMPETITION.md     # Plan complet
â”œâ”€â”€ STATUS.md                            # Statut dÃ©taillÃ©
â”œâ”€â”€ GUIDE-NEXT-STEPS.md                  # Ce fichier
â”‚
â”œâ”€â”€ price-scraper/                       # Projet scraper âœ…
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.ts                     # Point d'entrÃ©e âœ…
â”‚   â”‚   â”œâ”€â”€ test-setup.ts               # Test validation âœ…
â”‚   â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”‚   â”œâ”€â”€ base-scraper.ts         âœ…
â”‚   â”‚   â”‚   â”œâ”€â”€ swish-scraper.ts        âœ…
â”‚   â”‚   â”‚   â”œâ”€â”€ grainger-scraper.ts     âœ…
â”‚   â”‚   â”‚   â”œâ”€â”€ cleanitsupply-scraper.ts âœ…
â”‚   â”‚   â”‚   â”œâ”€â”€ uline-scraper.ts        âŒ Ã€ crÃ©er
â”‚   â”‚   â”‚   â””â”€â”€ ... (9 autres)          âŒ
â”‚   â”‚   â”œâ”€â”€ exporters/
â”‚   â”‚   â”‚   â””â”€â”€ excel-exporter.ts       âŒ Ã€ crÃ©er
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ prepare-commercial-products.ts   âœ…
â”‚   â”œâ”€â”€ consolidate-prices.ts            âŒ Ã€ crÃ©er
â”‚   â””â”€â”€ analyze-results.ts               âŒ Ã€ crÃ©er
â”‚
â””â”€â”€ results/                             # Vide (Ã  gÃ©nÃ©rer)
    â”œâ”€â”€ prix-par-site/
    â””â”€â”€ prix-consolides/
```

---

## ğŸš¨ Points d'Attention Critiques

### 1. SÃ©lecteurs CSS Fictifs (URGENT)
**Status:** âš ï¸ **BLOQUANT**
**Action:** Inspecter les 3 sites pilotes et ajuster `competitors-config.json`
**Temps:** 2-3 heures
**PrioritÃ©:** **MAXIMALE** - Rien ne fonctionnera sans cela

### 2. Anti-Scraping
Certains sites peuvent bloquer:
- **Solution:** Rate limiting strict respectÃ© (2-3s)
- **Si 403 errors:** Augmenter dÃ©lai Ã  4-5s
- **Si persistant:** Ajouter proxies (hors scope actuel)

### 3. Structures HTML Variables
Chaque site a sa propre structure. Les scrapers devront Ãªtre ajustÃ©s individuellement.

### 4. Tests RÃ©els Requis
Les scrapers n'ont **pas encore Ã©tÃ© testÃ©s sur les vrais sites**. Des ajustements seront nÃ©cessaires.

---

## ğŸ“ Commandes Utiles

```bash
# Navigation
cd Dissan/price-scraper

# Test de validation
npx tsx src/test-setup.ts

# Test sur un site spÃ©cifique (aprÃ¨s ajustement sÃ©lecteurs)
npm run scrape:site swish
npm run scrape:site grainger
npm run scrape:site cleanitsupply

# Voir les rÃ©sultats
cat ../results/prix-par-site/swish-results.json | jq

# Voir les logs
tail -f data/logs/swish-2024-11-18.log
tail -f data/logs/swish-2024-11-18-errors.log

# Voir les checkpoints
ls -lh data/checkpoints/

# Une fois tous les scrapers implÃ©mentÃ©s
npm run scrape:all              # Tout scraper (30-40h)
npm run scrape:priority1        # Sites prioritÃ© 1
npm run analyze                 # GÃ©nÃ©rer Excel consolidÃ©
```

---

## ğŸ¯ Checklist Avant Scraping Complet

- [ ] **Ã‰TAPE 1:** SÃ©lecteurs CSS ajustÃ©s pour Swish
- [ ] **Ã‰TAPE 1:** SÃ©lecteurs CSS ajustÃ©s pour Grainger
- [ ] **Ã‰TAPE 1:** SÃ©lecteurs CSS ajustÃ©s pour CleanItSupply
- [ ] **Ã‰TAPE 2:** Test sur 50 produits rÃ©ussi (taux >80%)
- [ ] **Ã‰TAPE 3:** 10 scrapers restants implÃ©mentÃ©s
- [ ] **Ã‰TAPE 3:** Tous les scrapers testÃ©s individuellement
- [ ] **Ã‰TAPE 4:** ExcelExporter crÃ©Ã©
- [ ] **Ã‰TAPE 4:** Script de consolidation crÃ©Ã©
- [ ] **Ã‰TAPE 5:** PrÃªt pour scraping complet

---

## ğŸ’¡ Conseils Pratiques

### Pour Ajuster les SÃ©lecteurs CSS:
1. Ouvrir DevTools (F12) sur le site
2. Utiliser l'inspecteur (Ctrl+Shift+C)
3. Tester les sÃ©lecteurs dans la console:
   ```javascript
   document.querySelectorAll('.product-item')
   ```
4. PrÃ©fÃ©rer les sÃ©lecteurs stables (classes, IDs) aux sÃ©lecteurs fragiles (nth-child)

### Pour DÃ©bugger un Scraper:
1. Mettre `headless: false` dans `config.ts` (ligne SCRAPING_CONFIG)
2. Lancer le scraper et observer le navigateur
3. VÃ©rifier les logs: `data/logs/{site}-{date}.log`
4. Utiliser console.log dans le scraper pour dÃ©bugger

### Pour Optimiser les Performances:
1. Scraper plusieurs sites en parallÃ¨le (si serveur puissant)
2. Augmenter `checkpointInterval` Ã  100 (moins de I/O)
3. DÃ©sactiver les logs debug en production

---

## ğŸ“Š Estimation Temps Total Restant

| Phase | Temps | Description |
|-------|-------|-------------|
| Ã‰TAPE 1 | 2-3h | Ajuster sÃ©lecteurs CSS (3 sites pilotes) |
| Ã‰TAPE 2 | 1-2h | Tester et valider (50 produits Ã— 3 sites) |
| Ã‰TAPE 3 | 6-8h | ImplÃ©menter 10 scrapers restants |
| Ã‰TAPE 4 | 3-4h | Excel exporter + consolidation |
| Ã‰TAPE 5 | 30-40h | **ExÃ©cution automatique** (peut tourner la nuit) |
| **TOTAL** | **12-17h actif** + **30-40h passif** |

**Temps dÃ©veloppement actif:** ~12-17 heures (sur 2-3 jours ouvrables)
**Temps exÃ©cution automatique:** ~30-40 heures (weekend/nuit)

---

## ğŸ‰ RÃ©sumÃ©

Vous avez maintenant une **infrastructure complÃ¨te et opÃ©rationnelle** pour scraper les prix de 576 produits sur 13 sites compÃ©titeurs canadiens.

**Prochaine action immÃ©diate:** Inspecter swish.ca avec DevTools et ajuster les sÃ©lecteurs CSS dans `competitors-config.json`.

Bonne chance! ğŸš€

---

**Questions? Consultez:**
- Plan complet: `PLAN-ANALYSE-PRIX-COMPETITION.md`
- Status dÃ©taillÃ©: `STATUS.md`
- README scraper: `price-scraper/README.md`
